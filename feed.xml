<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://masoudmim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://masoudmim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-01T19:53:01+00:00</updated><id>https://masoudmim.github.io/feed.xml</id><title type="html">Masoud Masoumi</title><subtitle>Masoud Masoumi&apos;s personal website </subtitle><entry><title type="html">Dynamic Mode Decomposition for Fluid Dynamics</title><link href="https://masoudmim.github.io/blog/2025/svd/" rel="alternate" type="text/html" title="Dynamic Mode Decomposition for Fluid Dynamics"/><published>2025-01-11T21:35:00+00:00</published><updated>2025-01-11T21:35:00+00:00</updated><id>https://masoudmim.github.io/blog/2025/svd</id><content type="html" xml:base="https://masoudmim.github.io/blog/2025/svd/"><![CDATA[<blockquote> <p><strong>Author’s Note:</strong> This post is based on a class project that I assigned to students in the Data-Driven Problem Solving course, which I taught in the Mechanical Engineering Department at the Cooper Union for the Advancement of Science and Art in Fall 2024. You can download the data for this post from <a href="https://github.com/MasoudMiM/masoudmim.github.io/blob/master/assets/data/cylinder_flow_data.npy">this link</a>.</p> </blockquote> <h2 id="theoretical-background">Theoretical Background</h2> <p>Before we dive into the specifics of SVD and DMD, let’s understand why we need these data-driven modeling techniques:</p> <ol> <li> <p><strong>Complex Systems</strong>: Many real-world systems, like fluid flows, are incredibly complex. Traditional physics-based models can be too complicated or computationally expensive to solve.</p> </li> <li> <p><strong>Data Abundance</strong>: We often have lots of data from sensors or simulations, but we need ways to make sense of it all.</p> </li> <li> <p><strong>Pattern Discovery</strong>: Data-driven methods can help us find hidden patterns or structures in our data that we might not see otherwise.</p> </li> <li> <p><strong>Prediction</strong>: Once we understand the patterns, we can use them to make predictions about how the system will behave in the future.</p> </li> <li> <p><strong>Simplification</strong>: These methods can help us simplify complex systems, focusing on the most important aspects.</p> </li> </ol> <p>Now, let’s look at a powerful data-driven modeling techniques: <strong>Dynamic Mode Decomposition</strong> (DMD). First, we look at Singular Value Decomposition (SVD) concept and then expand that to DMD.</p> <h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3> <p>Think of SVD as a way to break down a complicated puzzle into simpler pieces.</p> <ol> <li> <p><strong>What it does</strong>: SVD takes a big, complex matrix (like a puzzle) and breaks it into three simpler matrices.</p> </li> <li> <p><strong>The Math</strong>: If A is our data matrix, SVD says we can write it as:</p> <p>A = U * Σ * V^T</p> <p>Where:</p> <ul> <li>U and V are like the puzzle’s edge pieces and corner pieces</li> <li>Σ (Sigma) is like the importance of each piece</li> </ul> </li> <li> <p><strong>Simple Example</strong>: Imagine you have data about students’ heights and weights. SVD might show you that there’s one main pattern (taller students tend to weigh more) and some smaller patterns.</p> </li> <li> <p><strong>Why it’s useful</strong>:</p> <ul> <li>It helps us find the most important patterns in our data.</li> <li>We can use it to reduce noise in our data.</li> <li>It’s a key step in many other techniques, including DMD.</li> </ul> </li> </ol> <p>If you want to get a better understading of the concept of SVD, I strongly recommend reading <a href="https://gregorygundersen.com/blog/2018/12/10/svd/">this short post</a> and then watching these two, videos explaining the applications and mathematical derivation: - <a href="https://www.youtube.com/watch?v=gXbThCXjZFM">Singular Value Decomposition (SVD): Overview</a> - <a href="https://www.youtube.com/watch?v=nbBvuuNVfco">Singular Value Decomposition (SVD): Mathematical Overview</a></p> <p>If you want to dive deeper, here is <a href="https://www.youtube.com/watch?v=vSczTbgc8Rc">another video</a> that I suggest watching, which requires some background in linear algebra and matrix operations.</p> <h3 id="dynamic-mode-decomposition-dmd">Dynamic Mode Decomposition (DMD)</h3> <p>DMD is like a video editor for your data, helping you find repeating patterns over time.</p> <ol> <li> <p><strong>What it does</strong>: DMD looks at how your system changes over time and tries to find the main “actors” (modes) and their “scripts” (how they change).</p> </li> <li> <p><strong>The Math</strong>: If we have a series of data snapshots X₁, X₂, X₃, …, DMD assumes there’s some matrix A such that:</p> <p>X₂ ≈ A * X₁</p> <p>X₃ ≈ A * X₂</p> <p>And so on…</p> </li> <li> <p><strong>Simple Example</strong>: Imagine watching waves on a beach. DMD might find that there’s a big, slow wave (one mode) and smaller, faster ripples (other modes).</p> </li> <li><strong>How it works</strong>: <ol> <li>We arrange our data into two matrices: X₁ (earlier times) and X₂ (later times)</li> <li>We use SVD on X₁ to break it down: X₁ = U * Σ * V^T</li> <li>We use this to find our A matrix: A ≈ X₂ * V * Σ⁻¹ * U^T</li> <li>The eigenvectors of A are our modes, and the eigenvalues tell us how these modes change over time</li> </ol> </li> <li><strong>Why it’s useful</strong>: <ul> <li>It can find patterns that repeat over time.</li> <li>It can help us predict future behavior.</li> <li>It works well even with complex, nonlinear systems.</li> </ul> </li> </ol> <p>If you want to know more about DMD, I suggest watching <a href="https://www.youtube.com/watch?v=sQvrK8AGCAo">this video</a>. However, keep in mind that the complete understanding requires some background in linear algebra.</p> <h3 id="putting-it-all-together">Putting It All Together</h3> <p>Imagine you’re studying a river (our complex system). You take many photos over time (your data). SVD helps you identify the main features of the river (like bends or rapids). DMD then shows you how these features change over time (like how water flows around a bend). With this information, you can better understand the river’s behavior and even predict how it might change in the future.</p> <p>These techniques allow us to make sense of complex systems using data, even when we don’t fully understand all the underlying physics. They’re powerful tools in many fields, from fluid dynamics to finance to neuroscience.</p> <h2 id="dmd-analysis-of-2d-fluid-flow-around-a-cylinder">DMD Analysis of 2D Fluid Flow Around a Cylinder</h2> <p>In this post, we’ll apply DMD to model the fluid flow around a cylinder in 2D, a classic problem in fluid dynamics. By the end of this post, you’ll have implemented DMD from scratch and gained insights into how it can be used to analyze fluid flow patterns.</p> <h3 id="step-1-data-preparation">Step 1: Data Preparation</h3> <p>In this step, we’ll prepare the data for our DMD analysis. We’ll use a pre-generated dataset of fluid flow around a cylinder.</p> <h4 id="task-11-load-and-explore-the-data">Task 1.1: Load and Explore the Data</h4> <p>First, we write a function to load the fluid flow data and explore its basic properties. This step is crucial for understanding the structure of our data before we apply DMD.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">load_fluid_flow_data</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Load fluid flow data from a file and return it as a numpy array.
    
    Parameters:
    file_path (str): Path to the data file
    
    Returns:
    np.array: 3D array of shape (n_timesteps, n_x, n_y) containing fluid flow data
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def load_fluid_flow_data(file_path):
    """
    Load fluid flow data from a file and return it as a numpy array.
    Parameters:
    file_path (str): Path to the data file
    Returns:
    np.array: 3D array of shape (n_timesteps, n_x, n_y) containing fluid flow data
    """
    try:
        data = np.load(file_path)
        if data.ndim != 3:
            raise ValueError("Expected 3D array, but got array with {} dimensions".format(data.ndim))
        return data
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except Exception as e:
        print(f"Error loading data: {str(e)}")
        return None
</code></pre> </details> <p><br/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_flow_field</span><span class="p">(</span><span class="n">flow_data</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot the flow field at a given timestep.
    NOTE:
        - use imshow from matplotlib library
        - use extent=[-2, 4, -1, 1] and aspect=</span><span class="sh">'</span><span class="s">equal</span><span class="sh">'</span><span class="s"> options in your imshow plot to get better visulizations


    Parameters:
    flow_data (np.array): 3D array of fluid flow data
    timestep (int): Timestep to plot
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def plot_flow_field(flow_data, timestep):
    """
    Plot the flow field at a given timestep.
    Parameters:
    flow_data (np.array): 3D array of fluid flow data
    timestep (int): Timestep to plot
    """
    plt.figure(figsize=(10, 8))
    plt.imshow(flow_data[timestep], cmap='viridis', extent=[-2, 4, -1, 1])
    plt.colorbar(label='Flow Magnitude')
    plt.title(f'Flow Field at Timestep {timestep}')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()
</code></pre> </details> <p><br/></p> <p>Try the following code to test the functions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="nf">load_fluid_flow_data</span><span class="p">(</span><span class="sh">'</span><span class="s">cylinder_flow_data.npy</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">plot_flow_field</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">timestep</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>If we want to create an animtation of the data, we can write a separate function to achieve that goal:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">animate_flow</span><span class="p">(</span><span class="n">flow_data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Create an animation of the fluid flow.
    Parameters:
    flow_data (np.array): 3D array of fluid flow data
    interval (int): Delay between frames in milliseconds
    Returns:
    matplotlib.animation.FuncAnimation: Animation object
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def animate_flow(flow_data, interval=50):
    """
    Create an animation of the fluid flow.
    Parameters:
    flow_data (np.array): 3D array of fluid flow data
    interval (int): Delay between frames in milliseconds
    Returns:
    matplotlib.animation.FuncAnimation: Animation object
    """
    fig, ax = plt.subplots()
    # Plot the initial frame
    im = ax.imshow(flow_data[0], cmap='coolwarm', animated=True, extent=[-2, 4, -1, 1])
    def update(frame):
        im.set_array(flow_data[frame])
        return [im]
    
    anim = FuncAnimation(fig, update, frames=flow_data.shape[0],
    interval=interval, blit=True)
    plt.colorbar(im)
    plt.title("Fluid Flow Animation")
    return anim

</code></pre> </details> <p><br/></p> <p>You can use the following code snippet to check the function’s output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="nf">load_fluid_flow_data</span><span class="p">(</span><span class="sh">'</span><span class="s">cylinder_flow_data.npy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">anim</span> <span class="o">=</span> <span class="nf">animate_flow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="text-align: center;"> <img src="/assets/img/2dFlow.png" alt="animation of 2D flow" width="500"/> </div> <p><br/></p> <h4 id="task-12-reshape-the-data-for-dmd">Task 1.2: Reshape the Data for DMD</h4> <p>DMD requires the data to be in a specific format. We need to reshape our 3D flow field data (time, x, y) into a 2D matrix where each column represents a flattened snapshot of the flow field at a particular time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reshape_for_dmd</span><span class="p">(</span><span class="n">flow_data</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Reshape 3D flow data into a 2D matrix suitable for DMD.

    Parameters:
    flow_data (np.array): 3D array of shape (n_timesteps, n_x, n_y)

    Returns:
    np.array: 2D array of shape (n_x * n_y, n_timesteps)
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def reshape_for_dmd(flow_data):
    """
    Reshape 3D flow data into a 2D matrix suitable for DMD.

    Parameters:
    flow_data (np.array): 3D array of shape (n_timesteps, n_x, n_y)

    Returns:
    np.array: 2D array of shape (n_x * n_y, n_timesteps)
    """
    n_timesteps, n_x, n_y = flow_data.shape
    return flow_data.reshape(n_timesteps, -1).T
</code></pre> </details> <p><br/></p> <h3 id="step-2-implementing-the-dmd-algorithm">Step 2: Implementing the DMD Algorithm</h3> <p>In this step, we’ll implement the core DMD algorithm. We’ll break it down into smaller functions to make it easier to understand and implement.</p> <h4 id="task-21-compute-the-dmd-matrices">Task 2.1: Compute the DMD Matrices</h4> <p>Lets start with creating the matrices X1 and X2 needed for DMD analysis, and then compute the SVD of X1.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_dmd_matrices</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compute the matrices needed for DMD analysis.
    
    Parameters:
    X (np.array): 2D data matrix of shape (n_features, n_samples)
    
    Returns:
    tuple: (X1, X2) where X1 is X[:, :-1] and X2 is X[:, 1:]
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_svd</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compute the Singular Value Decomposition (SVD) of a matrix.
    
    Parameters:
    X (np.array): 2D matrix
    
    Returns:
    tuple: (U, S, Vt) - The SVD components
    </span><span class="sh">"""</span>
    <span class="c1"># Use numpy's SVD function    
</span>    <span class="c1"># The SVD decomposition gives X = U * S * Vt, where:
</span>    <span class="c1"># U: Left singular vectors (columns are orthonormal)
</span>    <span class="c1"># S: Singular values (diagonal matrix, but returned as 1D array)
</span>    <span class="c1"># Vt: Right singular vectors (rows are orthonormal)
</span>    
    <span class="c1"># Note: full_matrices=False returns the compact SVD, which is more efficient for DMD
</span>    
    <span class="c1"># Your code here
</span>
    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solutions</summary> <pre><code>
def compute_dmd_matrices(X):
    """
    Compute the matrices needed for DMD analysis.
    
    Parameters:
    X (np.array): 2D data matrix of shape (n_features, n_samples)
    
    Returns:
    tuple: (X1, X2) where X1 is X[:, :-1] and X2 is X[:, 1:]
    """
    X1 = X[:, :-1]
    X2 = X[:, 1:]
    return X1, X2

def compute_svd(X):
    """
    Compute the Singular Value Decomposition (SVD) of a matrix.
    
    Parameters:
    X (np.array): 2D matrix
    
    Returns:
    tuple: (U, S, Vt) - The SVD components
    """
    # Use numpy's SVD function    
    # The SVD decomposition gives X = U * S * Vt, where:
    # U: Left singular vectors (columns are orthonormal)
    # S: Singular values (diagonal matrix, but returned as 1D array)
    # Vt: Right singular vectors (rows are orthonormal)
    
    # Note: full_matrices=False returns the compact SVD, which is more efficient for DMD
    
    U, S, Vt = np.linalg.svd(X, full_matrices=False)
    return U, S, Vt
</code></pre> </details> <p><br/></p> <p>You can use the following lines to check the functions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># Testing compute_dmd_matrices
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="nf">compute_dmd_matrices</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">X1</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="sh">"</span><span class="s">X1 should have one less column than X</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">X2</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="sh">"</span><span class="s">X2 should have one less column than X</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="sh">"</span><span class="s">X1 should be all but the last column of X</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_equal</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="sh">"</span><span class="s">X2 should be all but the first column of X</span><span class="sh">"</span>

<span class="c1"># Testing compute_svd
</span><span class="n">X_svd_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="nf">compute_svd</span><span class="p">(</span><span class="n">X_svd_test</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">U</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="sh">"</span><span class="s">U should have the same number of rows as X and min(X.shape) columns</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">S</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="sh">"</span><span class="s">S should have length min(X.shape)</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">Vt</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="sh">"</span><span class="s">Vt should be square with size min(X.shape)</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">U</span> <span class="o">*</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span><span class="p">),</span> <span class="n">X_svd_test</span><span class="p">),</span> <span class="sh">"</span><span class="s">SVD decomposition should satisfy X = U * S * Vt</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">U</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">)),</span> <span class="sh">"</span><span class="s">U should be orthonormal</span><span class="sh">"</span>
<span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Vt</span><span class="p">,</span> <span class="n">Vt</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">)),</span> <span class="sh">"</span><span class="s">Vt should be orthonormal</span><span class="sh">"</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">All tests passed successfully!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="task-22-compute-dmd-modes-and-eigenvalues">Task 2.2: Compute DMD Modes and Eigenvalues</h4> <p>Now, we’ll use the results of the SVD to compute the DMD modes and eigenvalues. These represent the fundamental patterns and their temporal evolution in our fluid flow system.</p> <p>The theory behind computing DMD modes and eigenvalues is as follows:</p> <ol> <li> <p>Given the SVD of X_1 (U, S, Vt) and X_2, we can approximate the linear operator A that maps X_1 to X_2:</p> <p>A ≈ X₂ * V * S^(-1) * U^T</p> </li> <li> <p>We then compute the eigendecomposition of A:</p> <p>AW = WΛ</p> <p>where W are the eigenvectors and Λ are the eigenvalues.</p> </li> <li> <p>The DMD modes Φ are then given by:</p> <p>Φ = X₂ * V * S^(-1) * W</p> </li> <li> <p>The DMD eigenvalues are simply the eigenvalues Λ of A.</p> </li> </ol> <p>These DMD modes and eigenvalues capture the dominant spatial and temporal patterns in the data, respectively. The modes represent spatial structures, while the eigenvalues determine how these structures evolve over time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_dmd_modes_and_eigenvalues</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span><span class="p">,</span> <span class="n">X2</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compute the DMD modes and eigenvalues.

    Parameters:
    U, S, Vt (np.array): SVD components of X1
    X2 (np.array): Second snapshot matrix

    Returns:
    tuple: (modes, eigenvalues)
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def compute_dmd_modes_and_eigenvalues(U, S, Vt, X2):
    """
    Compute the DMD modes and eigenvalues.

    Parameters:
    U, S, Vt (np.array): SVD components of X1
    X2 (np.array): Second snapshot matrix

    Returns:
    tuple: (modes, eigenvalues)
    """
    # Compute the pseudoinverse of S
    S_inv = np.linalg.pinv(np.diag(S))
    
    # Compute the reduced DMD matrix
    A_tilde = U.T @ X2 @ Vt.T @ S_inv
    
    # Compute eigenvalues and eigenvectors of A_tilde
    eigenvalues, W = np.linalg.eig(A_tilde)
    
    # Compute the DMD modes
    modes = X2 @ Vt.T @ S_inv @ W
    
    return modes, eigenvalues
</code></pre> </details> <p><br/></p> <h3 id="step-3-applying-dmd-to-fluid-flow-data">Step 3: Applying DMD to Fluid Flow Data</h3> <p>Now that we have implemented the core DMD algorithm, let’s apply it to our fluid flow data.</p> <h4 id="task-31-perform-dmd-on-fluid-flow-data">Task 3.1: Perform DMD on Fluid Flow Data</h4> <p>We write a function that combines all the previous steps to perform DMD on the fluid flow data. This function will give us the modes and eigenvalues that characterize our fluid flow system.</p> <p>To develop this function, we need to understand the theory behind the DMD process:</p> <ol> <li>Data Preparation: <ul> <li>Reshape the 3D flow_data array into a 2D matrix X, where each column represents a flattened snapshot.</li> <li>Split X into X1 (all columns except the last) and X2 (all columns except the first).</li> </ul> </li> <li>Singular Value Decomposition (SVD): <ul> <li>Perform SVD on X1: X1 = U * S * Vt</li> <li>Truncate the SVD to the first r modes for dimensionality reduction.</li> </ul> </li> <li> <p>Compute the DMD operator:</p> <ul> <li>Ã = U_r^T * X₂ * V_r * S_r^(-1)</li> </ul> <p>where U_r, V_r, and S_r are the truncated versions of U, V, and S.</p> </li> <li> <p>Eigendecomposition of Ã:</p> <ul> <li>Solve the eigenvalue problem: Ã * W = W * Λ where W are the eigenvectors and Λ are the eigenvalues.</li> </ul> </li> <li>Compute DMD modes: <ul> <li>Φ = X₂ * V_r * S_r^(-1) * W</li> </ul> </li> <li>Compute DMD dynamics: <ul> <li>Calculate the continuous-time eigenvalues: ω = log(Λ) / Δt, where Δt is the time step between snapshots (assumed to be 1/100 in this case).</li> <li>Create a time vector t spanning the number of snapshots.</li> <li>Compute the time dynamics: exp(ω * t), which can be efficiently calculated using np.exp(np.outer(ω, t)).</li> <li>The resulting dynamics matrix will have dimensions (number of modes, number of timesteps).</li> </ul> </li> </ol> <p>With this theory in mind, we can now implement the function to perform DMD on the fluid flow data. We can implement <code class="language-plaintext highlighter-rouge">reshape_for_dmd</code>, <code class="language-plaintext highlighter-rouge">compute_dmd_matrices</code>, <code class="language-plaintext highlighter-rouge">compute_svd</code>, and <code class="language-plaintext highlighter-rouge">compute_dmd_modes_and_eigenvalues</code> functions within this function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">perform_dmd</span><span class="p">(</span><span class="n">flow_data</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Perform DMD on the fluid flow data.

    Parameters:
    flow_data (np.array): 3D array of fluid flow data
    r (int): Number of modes to retain

    Returns:
    tuple: (modes, eigenvalues, dynamics)
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def perform_dmd(flow_data, r=10):
    """
    Perform DMD on the fluid flow data.

    Parameters:
    flow_data (np.array): 3D array of fluid flow data
    r (int): Number of modes to retain

    Returns:
    tuple: (modes, eigenvalues, dynamics)
    """
    # Reshape the data into a 2D matrix
    X = reshape_for_dmd(flow_data) 

    # Split the data into two snapshot matrices
    X1, X2 = compute_dmd_matrices(X)

    # Perform SVD on X1
    U, S, Vt = compute_svd(X1)

    # Truncate to r modes
    U_r = U[:, :r]
    S_r = S[:r]
    Vt_r = Vt[:r, :]

    # Compute DMD modes and eigenvalues
    modes, eigenvalues = compute_dmd_modes_and_eigenvalues(U_r, S_r, Vt_r, X2)

    # Compute mode dynamics
    n_timesteps = flow_data.shape[0]
    dt = 1/100  # Assuming 1/100 time step, adjust if necessary
    omega = np.log(eigenvalues) / dt
    t = np.arange(n_timesteps) * dt
    dynamics = np.exp(np.outer(omega, t))

    return modes, eigenvalues, dynamics
</code></pre> </details> <p><br/></p> <h3 id="step-4-visualizing-and-interpreting-results">Step 4: Visualizing and Interpreting Results</h3> <p>In this final step, we’ll visualize the DMD results and interpret what they mean for our fluid flow system.</p> <h4 id="task-41-visualize-dmd-modes">Task 4.1: Visualize DMD Modes</h4> <p>In this task, we’ll visualize the DMD modes and their corresponding frequencies. This will help us understand the spatial patterns and temporal behavior of the dominant fluid flow structures.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_dmd_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot a single DMD mode for fluid flow around a cylinder.
    
    Parameters:
    -----------
    mode (np.array): 1D array representing a DMD mode
        Complex-valued array that will be reshaped to 2D
        Only the real part will be visualized
    
    shape (tuple): Original shape of the flow field (n_x, n_y)
        Dimensions to reshape the mode into its 2D representation

    Expected Output:
    ---------------
    A single figure (10x8 inches) showing:
    - 2D visualization of the DMD mode
    - Grayscale colormap
    - Physical domain extent: x=[-2,4], y=[-1,1]
    - Cylinder shown as black circle at origin (radius=0.2)
    - Colorbar showing mode magnitude
    - Axis labels </span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="s"> and </span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="s">
    - Title </span><span class="sh">"</span><span class="s">DMD Mode</span><span class="sh">"</span><span class="s">
    
    Visualization Details:
    ---------------------
    - Use imshow with aspect=</span><span class="sh">'</span><span class="s">equal</span><span class="sh">'</span><span class="s">
    - Include cylinder as black circle at (0,0)
    - Show real part of mode only
    - Maintain physical dimensions using extent parameter
    - Add colorbar with label </span><span class="sh">'</span><span class="s">Mode Magnitude</span><span class="sh">'</span><span class="s">

    Note:
    -----
    The visualization represents the spatial pattern of the mode
    in the physical domain around the cylinder, with darker and
    lighter regions showing the mode</span><span class="sh">'</span><span class="s">s structure.
    </span><span class="sh">"""</span>
    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def plot_dmd_mode(mode, shape):
    """
    Plot a single DMD mode.
    
    Parameters:
    mode (np.array): 1D array representing a DMD mode
    shape (tuple): Original shape of the flow field (n_x, n_y)
    """
    import matplotlib.pyplot as plt
    import numpy as np

    # Reshape the mode to the original flow field shape
    mode_2d = mode.reshape(shape)

    # Create a figure and axis
    fig, ax = plt.subplots(figsize=(10, 8))

    # Plot the mode using imshow
    im = ax.imshow(np.real(mode_2d), cmap='gray', aspect='equal', 
                   extent=[-2, 4, -1, 1])

    # Add a colorbar
    plt.colorbar(im, ax=ax, label='Mode Magnitude')

    # Set title and labels
    ax.set_title('DMD Mode')
    ax.set_xlabel('x')
    ax.set_ylabel('y')

    # Add the cylinder
    circle = plt.Circle((0, 0), 0.2, fill=False, color='k')
    ax.add_artist(circle)

    plt.tight_layout()
    plt.show()
</code></pre> </details> <p><br/></p> <div style="text-align: center;"> <img src="/assets/img/DMDMode.png" alt="DMD Mode" width="500"/> </div> <p><br/></p> <h5 id="understanding-plot_dmd_mode">Understanding <code class="language-plaintext highlighter-rouge">plot_dmd_mode</code></h5> <p>The <code class="language-plaintext highlighter-rouge">plot_dmd_mode</code> function helps us visualize individual DMD modes. Here’s what it does in simple terms:</p> <ol> <li><strong>What is a DMD mode?</strong> <ul> <li>A DMD mode is a spatial pattern in the fluid flow that evolves over time.</li> <li>It represents a recurring structure or behavior in the flow.</li> </ul> </li> <li><strong>What does the plot show?</strong> <ul> <li>The plot shows a 2D image of a single DMD mode.</li> <li>The colors in the image represent the strength or importance of the mode at different locations.</li> <li>Brighter areas show where the mode has a stronger effect on the flow.</li> <li>Darker areas show where the mode has less influence.</li> </ul> </li> <li><strong>How to interpret the plot:</strong> <ul> <li>Look for patterns: Are there areas of high intensity? Do you see any symmetries or recurring structures?</li> <li>Compare with the original flow: How does this pattern relate to what you see in the overall flow?</li> <li>Consider the location: Are there strong patterns near the cylinder or in the wake behind it?</li> </ul> </li> <li><strong>Why is this useful?</strong> <ul> <li>It helps us identify important spatial structures in the flow.</li> <li>We can see where the most significant flow behaviors are occurring.</li> <li>By looking at multiple modes, we can build up a picture of the complex flow behavior.</li> </ul> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_mode_frequencies</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot the frequencies and growth rates of DMD modes.
    
    Parameters:
    -----------
    eigenvalues (np.array): DMD eigenvalues
        Complex-valued array containing temporal information
        Used to compute frequencies and growth rates
    
    dt (float): Time step between snapshots
        Time interval between data frames
        Used to convert eigenvalues to physical frequencies

    Expected Output:
    ---------------
    A single figure (10x5 inches) showing:
    - Scatter plot of mode frequencies vs growth rates
    - X-axis: Frequencies (computed as imag(log(λ))/(2π*dt))
    - Y-axis: Growth rates (computed as real(log(λ))/dt)
    - Red dashed line at y=0 separating growing/decaying modes
    - Grid lines for better readability
    
    Visualization Details:
    ---------------------
    - X-axis label: </span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="s">
    - Y-axis label: </span><span class="sh">"</span><span class="s">Growth Rate</span><span class="sh">"</span><span class="s">
    - Title: </span><span class="sh">"</span><span class="s">DMD Mode Frequencies and Growth Rates</span><span class="sh">"</span><span class="s">
    - Points above y=0: Growing modes
    - Points below y=0: Decaying modes

    Note:
    -----
    The plot helps identify:
    - Dominant frequencies in the flow
    - Stability of different modes (growing vs decaying)
    - Patterns in mode behavior
    </span><span class="sh">"""</span>
    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def plot_mode_frequencies(eigenvalues, dt):
    """
    Plot the frequencies of the DMD modes.
    
    Parameters:
    eigenvalues (np.array): DMD eigenvalues
    dt (float): Time step between snapshots
    """
    frequencies = np.log(eigenvalues).imag / (2 * np.pi * dt)
    growth_rates = np.log(eigenvalues).real / dt
    
    plt.figure(figsize=(10, 5))
    plt.scatter(frequencies, growth_rates)
    plt.xlabel('Frequency')
    plt.ylabel('Growth Rate')
    plt.title('DMD Mode Frequencies and Growth Rates')
    plt.axhline(y=0, color='r', linestyle='--')
    plt.grid(True)
    plt.show()
</code></pre> </details> <p><br/></p> <h5 id="understanding-plot_mode_frequencies">Understanding <code class="language-plaintext highlighter-rouge">plot_mode_frequencies</code></h5> <p>The <code class="language-plaintext highlighter-rouge">plot_mode_frequencies</code> function helps us visualize two important aspects of each DMD mode: its frequency and its growth rate.</p> <ol> <li><strong>Frequency:</strong> <ul> <li>Imagine a mode as a pattern in the fluid flow that repeats over time.</li> <li>The frequency tells us how quickly this pattern repeats.</li> <li>A higher frequency means the pattern repeats more often in a given time.</li> <li>For example, a high-frequency mode might represent rapid, small-scale fluctuations in the flow.</li> </ul> </li> <li><strong>Growth Rate:</strong> <ul> <li>The growth rate tells us whether the pattern is getting stronger or weaker over time.</li> <li>A positive growth rate means the pattern is amplifying (getting stronger).</li> <li>A negative growth rate means the pattern is decaying (getting weaker).</li> <li>A growth rate near zero means the pattern stays about the same strength over time.</li> </ul> </li> <li><strong>What does the plot show?</strong> <ul> <li>Each point on the plot represents one DMD mode.</li> <li>The horizontal axis (x-axis) shows the frequency of each mode.</li> <li>The vertical axis (y-axis) shows the growth rate of each mode.</li> <li>The red dashed line at y=0 separates growing modes (above the line) from decaying modes (below the line).</li> </ul> </li> <li><strong>How to interpret the plot:</strong> <ul> <li>Points on the right side represent high-frequency modes (fast-repeating patterns).</li> <li>Points on the left side represent low-frequency modes (slow-repeating patterns).</li> <li>Points above the red line are growing modes (getting stronger over time).</li> <li>Points below the red line are decaying modes (getting weaker over time).</li> <li>Points near the red line are relatively stable modes (neither growing nor decaying much).</li> </ul> </li> <li><strong>Why is this useful?</strong> <ul> <li>It helps us identify which patterns (modes) are most important in the fluid flow.</li> <li>We can see if there are any dominant frequencies in the flow.</li> <li>We can tell if the flow has any unstable patterns that might grow over time.</li> <li>It can help us understand the overall behavior of the fluid system.</li> </ul> </li> </ol> <p>In the context of fluid flow around a cylinder, you might see:</p> <ul> <li>Low-frequency modes representing the overall flow pattern around the cylinder.</li> <li>Higher-frequency modes that might correspond to vortex shedding behind the cylinder.</li> <li>Possibly some growing modes if the flow becomes unstable under certain conditions.</li> </ul> <p>Both <code class="language-plaintext highlighter-rouge">plot_dmd_mode</code> and <code class="language-plaintext highlighter-rouge">plot_mode_frequencies</code> provide a comprehensive understanding of the key patterns in your fluid flow simulation, how they’re distributed in space, and how they behave over time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing the functions using the first 2 modes
</span><span class="n">flow_data</span> <span class="o">=</span> <span class="nf">load_fluid_flow_data</span><span class="p">(</span><span class="sh">'</span><span class="s">cylinder_flow_data.npy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">modes</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">dynamics</span> <span class="o">=</span> <span class="nf">perform_dmd</span><span class="p">(</span><span class="n">flow_data</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nf">plot_dmd_mode</span><span class="p">(</span><span class="n">modes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">dt</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># Assuming time step of 0.01
</span><span class="nf">plot_mode_frequencies</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
</code></pre></div></div> <h4 id="task-42-reconstruct-and-compare-flow-fields">Task 4.2: Reconstruct and Compare Flow Fields</h4> <p>The theory behind reconstructing the flow field using DMD is as follows:</p> <ol> <li><strong>DMD Reconstruction:</strong> The DMD approximation of the flow field at time t is given by: x(t) ≈ Φ * b(t) where: <ul> <li>Φ are the DMD modes (spatial patterns)</li> <li>b(t) are the mode amplitudes at time t (stored in dynamics matrix)</li> </ul> </li> <li><strong>Matrix Implementation:</strong> For a specific timestep k: <ul> <li>Select the k-th column from dynamics matrix: b(k) = dynamics[:, k]</li> <li>Multiply modes with dynamics: x(k) = modes @ b(k)</li> <li>Take real part for physical solution</li> <li>Reshape to original flow field dimensions</li> </ul> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reconstruct_flow_field</span><span class="p">(</span><span class="n">modes</span><span class="p">,</span> <span class="n">dynamics</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Reconstruct the flow field at a given timestep using DMD.
    
    Parameters:
    -----------
    modes (np.array): DMD modes, shape (n_features, n_modes)
        Spatial patterns extracted by DMD
        Each column represents one spatial mode
    
    dynamics (np.array): DMD mode dynamics, shape (n_modes, n_timesteps)
        Temporal evolution of each mode
        Each row shows how one mode evolves over time
    
    timestep (int): Timestep to reconstruct
        Index of the time point to reconstruct
        Must be less than n_timesteps
    
    Returns:
    --------
    np.array: Reconstructed flow field, shape (n_x, n_x)
        2D array representing the reconstructed flow field
        Dimensions are square (n_x = sqrt(n_features))
        Contains real-valued flow field data

    Expected Steps:
    --------------
    1. Combine modes and dynamics at specified timestep
    2. Reshape result to square 2D array

    Note:
    -----
    - Input data should be properly normalized
    - Result will be real-valued (use np.real())
    - Assumes square flow field domain
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">compare_original_and_reconstructed</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">reconstructed</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compare the original and reconstructed flow fields visually and quantitatively.
    
    Parameters:
    -----------
    original (np.array): Original flow field, shape (n_x, n_x)
        2D array containing the original flow field data
        Used as ground truth for comparison
    
    reconstructed (np.array): Reconstructed flow field, shape (n_x, n_x)
        2D array containing the DMD-reconstructed flow field
        Should have same dimensions as original

    Expected Output:
    ---------------
    A single figure (12x5 inches) with:
    1. Left subplot:
       - Original flow field visualization
       - Title: </span><span class="sh">"</span><span class="s">Original Flow Field</span><span class="sh">"</span><span class="s">
       - Colormap: </span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="s">
       - Physical domain: x=[-2,4], y=[-1,1]
       - Colorbar showing flow values
    
    2. Right subplot:
       - Reconstructed flow field visualization
       - Title: </span><span class="sh">"</span><span class="s">Reconstructed Flow Field</span><span class="sh">"</span><span class="s">
       - Same colormap and domain as original
       - Colorbar showing flow values
    
    3. Overall figure:
       - Suptitle showing relative error
       - Equal aspect ratio for both plots
       - Tight layout

    Printed Metrics:
    ---------------
    - Mean Squared Error (MSE)
    - Mean Absolute Error (MAE)
    - Relative Error (norm of difference / norm of original)

    Note:
    -----
    - Both visualizations use same scale for fair comparison
    - Error metrics help quantify reconstruction quality
    - Lower error values indicate better reconstruction
    </span><span class="sh">"""</span>
    <span class="c1"># Your code here
</span>    <span class="k">pass</span>
</code></pre></div></div> <details> <summary>Click to see the solution</summary> <pre><code>
def reconstruct_flow_field(modes, dynamics, timestep):
    """
    Reconstruct the flow field at a given timestep using DMD.
    
    Parameters:
    modes (np.array): DMD modes
    dynamics (np.array): DMD mode dynamics
    timestep (int): Timestep to reconstruct
    
    Returns:
    np.array: Reconstructed flow field
    """
    # Reconstruct the flow field by combining modes and dynamics
    reconstructed = np.real(np.dot(modes, dynamics[:, timestep]))
    
    # Reshape the reconstructed field to match the original dimensions
    original_shape = int(np.sqrt(reconstructed.shape[0]))
    reconstructed = reconstructed.reshape((original_shape, original_shape))
    
    return reconstructed

def compare_original_and_reconstructed(original, reconstructed):
    """
    Compare the original and reconstructed flow fields.
    
    Parameters:
    original (np.array): Original flow field
    reconstructed (np.array): Reconstructed flow field
    """
    # Create a figure with two subplots side by side
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Plot original flow field
    im1 = ax1.imshow(original, cmap='coolwarm', aspect='equal', extent=[-2, 4, -1, 1])
    ax1.set_title('Original Flow Field')
    plt.colorbar(im1, ax=ax1)

    # Plot reconstructed flow field
    im2 = ax2.imshow(reconstructed, cmap='coolwarm', aspect='equal', extent=[-2, 4, -1, 1])
    ax2.set_title('Reconstructed Flow Field')
    plt.colorbar(im2, ax=ax2)

    # Compute and display the relative error
    relative_error = np.linalg.norm(original - reconstructed) / np.linalg.norm(original)
    plt.suptitle(f'Comparison (Relative Error: {relative_error:.4f})')

    plt.tight_layout()
    plt.show()

    # Print additional error metrics
    mse = np.mean((original - reconstructed)**2)
    mae = np.mean(np.abs(original - reconstructed))
    print(f'Mean Squared Error: {mse:.6f}')
    print(f'Mean Absolute Error: {mae:.6f}')
</code></pre> </details> <p><br/></p> <div style="text-align: center;"> <img src="/assets/img/Comparison.png" alt="Comparison between original and reconstructed" width="600"/> </div> <p><br/></p> <p>The following code sample can be used to test the functions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loading the original data
</span><span class="n">original_data</span> <span class="o">=</span> <span class="nf">load_fluid_flow_data</span><span class="p">(</span><span class="sh">'</span><span class="s">cylinder_flow_data.npy</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># Performing DMD 
</span><span class="n">modes</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">dynamics</span> <span class="o">=</span> <span class="nf">perform_dmd</span><span class="p">(</span><span class="n">original_data</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># r is the number of modes to retain
</span>
<span class="n">timestep</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># for example
</span>
<span class="c1"># Reconstructing the flow field
</span><span class="n">reconstructed_field</span> <span class="o">=</span> <span class="nf">reconstruct_flow_field</span><span class="p">(</span><span class="n">modes</span><span class="p">,</span> <span class="n">dynamics</span><span class="p">,</span> <span class="n">timestep</span><span class="p">)</span>
<span class="c1"># Comparing original and reconstructed fields
</span><span class="nf">compare_original_and_reconstructed</span><span class="p">(</span><span class="n">original_data</span><span class="p">[</span><span class="n">timestep</span><span class="p">],</span> <span class="n">reconstructed_field</span><span class="p">)</span>
</code></pre></div></div> <p>Interpreting the Outputs:</p> <ol> <li>Reconstructed Flow Field: <ul> <li>The output of <code class="language-plaintext highlighter-rouge">reconstruct_flow_field</code> is a 2D array representing the reconstructed flow field at the specified timestep.</li> <li>This field should resemble the original flow field, capturing the main features of the flow around the cylinder.</li> <li>The accuracy of the reconstruction depends on the number of modes used in the DMD analysis.</li> </ul> </li> <li>Comparison Visualization: <ul> <li>The <code class="language-plaintext highlighter-rouge">compare_original_and_reconstructed</code> function should produce a side-by-side plot of the original and reconstructed flow fields.</li> <li>Look for similarities in the overall structure of the flow, particularly around the cylinder and in the wake region.</li> <li>Pay attention to the color scales: they should be similar for both plots if the reconstruction is accurate.</li> </ul> </li> <li>Number of Modes: <ul> <li>Try reconstructing the flow field with different numbers of DMD modes.</li> <li>Generally, using more modes will improve the accuracy of the reconstruction but may also introduce noise or overfitting.</li> <li>Find a balance where the reconstruction captures the important flow features without including too much detail that might be noise.</li> </ul> </li> </ol> <p>By comparing the reconstructed flow field with the original data, you can assess how well the DMD analysis has captured the essential dynamics of the system. This comparison helps validate the DMD results and provides insight into which flow features are most significant in the overall dynamics.</p> <h3 id="conclusion">Conclusion</h3> <p>We’ve implemented the Dynamic Mode Decomposition algorithm from scratch and applied it to analyze 2D fluid flow around a cylinder. You’ve learned how to:</p> <ol> <li>Prepare and preprocess fluid flow data for DMD analysis</li> <li>Implement the core DMD algorithm, including SVD</li> <li>Apply DMD to extract dominant modes and dynamics from fluid flow data</li> <li>Visualize and interpret the results of DMD analysis</li> </ol> <p>I hope this post has given you hands-on experience with a powerful data-driven method for analyzing complex fluid systems. The skills you’ve developed can be applied to a wide range of problems in fluid dynamics and other fields of engineering.</p>]]></content><author><name></name></author><category term="technical"/><category term="SVD,"/><category term="DMD"/><summary type="html"><![CDATA[Author’s Note: This post is based on a class project that I assigned to students in the Data-Driven Problem Solving course, which I taught in the Mechanical Engineering Department at the Cooper Union for the Advancement of Science and Art in Fall 2024. You can download the data for this post from this link.]]></summary></entry><entry><title type="html">Embracing the Repair Mindset</title><link href="https://masoudmim.github.io/blog/2024/repair-minset/" rel="alternate" type="text/html" title="Embracing the Repair Mindset"/><published>2024-12-27T21:20:00+00:00</published><updated>2024-12-27T21:20:00+00:00</updated><id>https://masoudmim.github.io/blog/2024/repair-minset</id><content type="html" xml:base="https://masoudmim.github.io/blog/2024/repair-minset/"><![CDATA[<figure style="text-align: center; max-width: 500px; margin: 0 auto;"> <audio controls="" style="width: 100%;"> <source src="/assets/audio/repair_minset.mp3" type="audio/mpeg"/> Your browser does not support the audio element. </audio> <figcaption>(Listen to this article)</figcaption> </figure> <p><br/></p> <div style="text-align: justify;"> <p>I used to love tinkering with my toys as a kid, always curious about how things worked. There was something somewhat satisfying about taking apart a gadget and putting it back together, like I was unlocking a little mystery. I guess there’s a reason I ended up in the engineering field. Back then—and I’m really not that old!—handymen seemed genuinely engaged in their work, capable of fixing just about anything. Now, it feels like we’re living in a world where everything is designed to be tossed aside the moment it stops working. This is partly the fault of companies, with their often unnecessarily complex designs that make repairs seem not worth the time and cost, especially when relatively cheap replacement parts are readily available. With over 8 billion people on this planet, the pressure to consume is insane, and it’s taking a toll on our environment.</p> <p>Take smartphones, for example. Every year, millions of shiny new devices hit the market, and they’re often built to last just long enough for us to want the next big thing. I can’t tell you how many friends I’ve seen ditch perfectly good phones just because they wanted the latest model. I’ve been guilty of this too. It’s like we’ve forgotten how to appreciate what we have. I can’t help but think about how my childhood was filled with fixing things, not just throwing them away or replacing them with something new.</p> <p>The idea of replacing rather than repairing has increasingly penetrated our personal lives. In a world where everything revolves around speed—think online shopping and instant messaging—patience seems to be a thing of the past. When a friendship hits a rough patch, it’s all too easy to walk away instead of putting in the effort to mend it. It feels like we’ve adopted a “replace it” mindset for our connections, just as we do with our gadgets. Sure, technology has made life easier in many ways, but it has also made us a little lazy when it comes to nurturing our relationships. We’re so focused on quick fixes that we forget the beauty of taking our time to understand one another. It’s a shame because the effort we invest in repairing our bonds can potentially lead to some of the most rewarding experiences in life, admittedly not always though.</p> <p>So, what’s the takeaway? There might not be one! This was more of a personal reflection and perhaps a bit of an infatuation with nostalgia. However, I believe that as we navigate this fast-paced world, it’s important to remember the value of repair—both in our belongings and our relationships. Embracing the art of fixing things can help us cultivate deeper connections and a more sustainable lifestyle. It’s all about finding that balance between convenience and care. Let’s bring back the joy of repair, one gadget and one relationship at a time.</p> </div>]]></content><author><name></name></author><category term="essay"/><category term="repair,"/><category term="sustainability"/><summary type="html"><![CDATA[Your browser does not support the audio element. (Listen to this article)]]></summary></entry><entry><title type="html">Ideas Matter More Than Ever</title><link href="https://masoudmim.github.io/blog/2024/execution/" rel="alternate" type="text/html" title="Ideas Matter More Than Ever"/><published>2024-10-28T20:10:00+00:00</published><updated>2024-10-28T20:10:00+00:00</updated><id>https://masoudmim.github.io/blog/2024/execution</id><content type="html" xml:base="https://masoudmim.github.io/blog/2024/execution/"><![CDATA[<figure style="text-align: center; max-width: 500px; margin: 0 auto;"> <audio controls="" style="width: 100%;"> <source src="/assets/audio/ideasmatter.mp3" type="audio/mpeg"/> Your browser does not support the audio element. </audio> <figcaption>(Listen to this article)</figcaption> </figure> <p><br/></p> <p>“Ideas are cheap, execution is everything” - this saying has guided business thinking for many years. For decades, turning an idea into reality required specialized skills, substantial capital, and extensive networks. This high barrier to execution led to a natural emphasis on implementation over ideation.</p> <p>But artificial intelligence is fundamentally changing this equation. AI tools are democratizing the ability to execute, potentially shifting the balance between having great ideas and implementing them. This essay explores how AI is transforming this long-held paradigm and what it means for innovation in the future.</p> <p>The dominance of execution over ideas wasn’t arbitrary - it reflected real-world constraints. While groundbreaking ideas were valuable, history showed that superior execution often determined success. Facebook didn’t invent social networking, and Apple didn’t create the first smartphoneyet both companies achieved extraordinary success through exceptional implementation of existing concepts.</p> <p>This reality shaped how businesses operated. Success required:</p> <ul> <li>Specialized expertise across multiple domains</li> <li>Substantial financial resources</li> <li>Strong professional networks</li> <li>Significant time investment</li> <li>Deep market understanding</li> </ul> <p>These demanding requirements served as natural gatekeepers, distinguishing truly viable business ventures from mere conceptual proposals. While companies like Amazon and Tesla are often celebrated for their innovative ideas, their success stemmed from the rare ability to combine visionary concepts with exceptional execution capabilities. The high barriers to implementation meant that many potentially groundbreaking ideas never made it past the conceptual stage, simply because their creators lacked the resources and expertise needed for effective execution.</p> <p>Artificial intelligence is now dismantling these traditional barriers to execution. What once required teams of specialists can increasingly be accomplished through AI-powered tools:</p> <ul> <li>Technical Development: Complex tasks like coding, design, and content creation can now be performed by individuals with minimal technical expertise.</li> <li>Business Intelligence: Market research and competitive analysis are accessible through AI platforms that process vast amounts of data rapidly.</li> <li>Product Development: The journey from concept to prototype is accelerating dramatically, reducing both time and expertise requirements.</li> <li>Customer Engagement: Sophisticated marketing and customer relationship management tools are now available to entrepreneurs of all sizes.</li> </ul> <p>As these barriers fall, we’re witnessing a fundamental shift in how ideas can be brought to market. The democratization of execution capabilities may be creating a new paradigm where innovative ideas become the primary differentiator.</p> <p>As AI democratizes execution capabilities, we’re entering an era where the scarcity equation is inverting. When everyone has access to powerful execution tools, what becomes rare and valuable? The answer may be truly innovative ideas themselves.</p> <p>This shift manifests in several ways:</p> <ul> <li> <p>Market Differentiation: As AI lowers execution barriers, markets may become saturated with well-implemented but similar products. Novel ideas and unique approaches become critical differentiators.</p> </li> <li> <p>Innovation Acceleration: The reduced gap between concept and implementation enables rapid testing of new ideas. This acceleration creates opportunities for “idea entrepreneurs” who can quickly validate and iterate on innovative concepts.</p> </li> <li> <p>Value Creation: Traditional business models focused on execution excellence may give way to those centered on conceptual innovation. We might see the emergence of new marketplaces where ideas themselves become valuable tradable assets.</p> </li> <li> <p>Skill Evolution: The premium on execution-related skills may shift toward abilities that AI can’t easily replicate: creative thinking, problem identification, and novel concept generation.</p> </li> </ul> <p>This rebalancing doesn’t diminish the importance of execution but rather transforms it. AI handles the implementation details, allowing human creativity and innovative thinking to take center stage in the value creation process.</p> <p>While AI is democratizing execution capabilities, several significant challenges emerge in this new paradigm:</p> <ul> <li> <p>Human Elements Remain Critical: Despite AI’s capabilities, certain aspects of business remain distinctly human. Complex decision-making, relationship building, and understanding subtle market dynamics still require human judgment and emotional intelligence.</p> </li> <li> <p>The Homogenization Risk: When everyone has access to similar AI execution tools, we face the paradox of standardization. Products and services may become increasingly similar, potentially creating a new form of competition where standing out requires even more innovative thinking.</p> </li> <li>Ethical Considerations: The rise of AI-assisted execution raises important questions: <ul> <li>Intellectual property rights in AI-generated content</li> <li>Potential amplification of existing biases</li> <li>Fair attribution of value between human ideation and AI execution</li> <li>Privacy concerns in data-driven implementation</li> </ul> </li> <li> <p>The Digital Divide: While AI promises democratization, access to these technologies isn’t universal. This disparity could create new forms of inequality, where those with access to advanced AI tools gain disproportionate advantages in bringing ideas to market.</p> </li> <li>AI’s Role in Ideation: As AI systems become more sophisticated, they may begin to influence not just execution but ideation itself. This raises questions about the nature of creativity and innovation in an AI-augmented world.</li> </ul> <p>These challenges suggest that while AI is transforming the idea-execution dynamic, the transition isn’t straightforward. Success in this new paradigm requires carefully navigating these limitations while leveraging AI’s capabilities.</p> <p>The advent of AI is catalyzing a profound shift in the long-standing dynamic between ideas and execution. As artificial intelligence democratizes implementation capabilities, we’re witnessing an inversion of the traditional business maxim that “ideas are cheap, execution is everything.”</p> <p>This transformation isn’t merely technological - it’s reshaping the fundamental nature of innovation. In a world where AI can level the execution playing field, the scarcity of truly innovative ideas may outweigh the scarcity of implementation capabilities. However, this shift brings its own complexities: the risk of homogenization, ethical considerations, and the potential for new forms of inequality.</p> <p>Looking ahead, success will likely belong to those who can harness both AI’s execution capabilities and humanity’s unique capacity for creative thinking. The future may not belong to those who can execute better, but to those who can imagine better.</p>]]></content><author><name></name></author><category term="essay"/><category term="idea,"/><category term="innovation"/><summary type="html"><![CDATA[How artificial intelligence is inverting the traditional 'execution over ideas' paradigm in business and innovation]]></summary></entry><entry><title type="html">Transformers - Fundamental Concepts with Python Implementation</title><link href="https://masoudmim.github.io/blog/2024/transformers/" rel="alternate" type="text/html" title="Transformers - Fundamental Concepts with Python Implementation"/><published>2024-06-14T21:10:00+00:00</published><updated>2024-06-14T21:10:00+00:00</updated><id>https://masoudmim.github.io/blog/2024/transformers</id><content type="html" xml:base="https://masoudmim.github.io/blog/2024/transformers/"><![CDATA[<h4 id="table-of-contents">Table of Contents</h4> <ol> <li><a href="#motiv">Motivation - Why Transformers?</a></li> <li><a href="#term">Terminology</a> <ul> <li><a href="#softmax">Softmax</a></li> <li><a href="#tokeniz">Tokenization and Word Embedding</a></li> <li><a href="#qkv">Queries, Keys, and Values</a></li> <li><a href="#selfatt">Self Attention Mechanism</a></li> <li><a href="#norm">Normalization</a></li> <li><a href="#posenc">Positional Encoding</a></li> </ul> </li> <li><a href="#struc-example">Attention Mechanism: Learn by Example</a></li> <li><a href="#models">Transformer Models</a></li> <li><a href="#struc">Coding a Simple Transformer</a></li> <li><a href="#fincom">Final Comments</a></li> </ol> <h2 id="1-motivation---why-transformers-">1. Motivation - Why Transformers? <a name="motiv"></a></h2> <p>If you are interested in modern natural language processing (NLP), machine learning, and artificial intelligence, you probably need to learn <a href="https://arxiv.org/pdf/1706.03762">Transformers</a> and know how they can be implemented for various tasks. Transformers have set new benchmarks in a variety of NLP tasks such as language translation, text summarization, and question answering. Models like <a href="https://huggingface.co/docs/transformers/en/model_doc/bert">BERT</a>, <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a>, <a href="https://openai.com/index/gpt-4/">GPT-4</a>, <a href="https://huggingface.co/docs/transformers/en/model_doc/t5">T5</a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/llama">LLaMA</a>, and <a href="https://claude.ai/chats">Claude</a> are all implementing Transformers in their architectures. Transformers have also been adapted for use in other domains such as computer vision (<a href="https://huggingface.co/docs/transformers/en/model_doc/vit">Vision Transformers ViTs</a>) and <a href="https://huggingface.co/docs/transformers/en/model_doc/speech_to_text">speech processing</a>. One major advantage of Transformers, when compared with Recurrent Neural Networks, is their capability for parallelization.</p> <p>Understanding Transformers provides a foundation for delving into more advanced topics in AI and machine learning. Many current research directions and innovations build upon the Transformer architecture, making it essential knowledge for staying up-to-date in the field. Further, the popularity of Transformers has led to a wealth of resources, including research papers, tutorials, open-source implementations (like <a href="https://huggingface.co/docs/transformers/en/index">Hugging Face’s Transformers library</a>), and community support. This makes it easier to learn, experiment, and apply Transformer models in various projects.</p> <h2 id="2-terminology-">2. Terminology <a name="term"></a></h2> <p>Transformers use a mechanism called <strong>attention</strong> to determine the importance of different words in a sequence. The core components of this <strong>attention mechanism</strong> are <em>queries</em>, <em>keys</em>, and <em>values</em>. Let’s take a closer look at some fundamental components of Transformers first and familiarize ourselves with some important concepts.</p> <h3 id="softmax">softmax<a name="softmax"></a></h3> <p>Softmax function is a mathematical function that converts a vector of values into a probability distribution. It is widely used in machine learning, especially in classification tasks and attention mechanisms, because it transforms input values to be in the range (0, 1) and ensures they sum up to 1. The softmax function is defined as follows:</p> \[\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\] <p>where:</p> <ul> <li>\(x_i\)​ is the \(i^{th}\) element of the input vector.</li> <li>\(e\) is the base of the natural logarithm.</li> </ul> <p>and the denominator is the sum of the exponentials of all elements in the input vector. Here is a simple implementation in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
	
	<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_x</span><span class="p">)</span>
	
	<span class="c1"># Example input vector
</span>	<span class="n">input_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
	
	<span class="c1"># Compute softmax
</span>	<span class="n">softmax_output</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">input_vector</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Vector:</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Softmax Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">softmax_output</span><span class="p">)</span>
</code></pre></div></div> <p>which will return</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	Input Vector: <span class="o">[</span>1. 2. 3.]
	Softmax Output: <span class="o">[</span>0.09003057 0.24472847 0.66524096] 
</code></pre></div></div> <h3 id="tokenization-and-word-embedding">Tokenization and Word Embedding<a name="tokeniz"></a></h3> <p>This is the process of breaking down a text into smaller units, which are called <em>tokens</em>. These tokens can be words, subwords, or characters. Tokenization is an important step in natural language processing because it transforms raw text data into a format that can be processed and converted into numerical format later (using embedding process).</p> <p><em>Word embeddings</em> are dense vector representations of words in a high-dimensional space, where similar words are closer to each other in this space. This is where we convert words into <em>numerical</em> format, which is essential for computing purposes. Here is a simple Python code to get a better sense of tokenization procedure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">re</span>
	
	<span class="k">def</span> <span class="nf">simple_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
	    <span class="c1"># Define a regular expression pattern for tokenization
</span>	    <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">(\b\w+\b|[.,!?;])</span><span class="sh">'</span><span class="p">)</span>
	    <span class="c1"># Use findall to get all matches
</span>	    <span class="n">tokens</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
	    <span class="k">return</span> <span class="n">tokens</span>
	
	<span class="c1"># Example text
</span>	<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Hello, how are you doing today?</span><span class="sh">"</span>
	
	<span class="c1"># Tokenize the text
</span>	<span class="n">tokens</span> <span class="o">=</span> <span class="nf">simple_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
	
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original Text:</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Tokens:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p>with the output</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original Text: Hello, how are you doing today?
Tokens: <span class="o">[</span><span class="s1">'Hello'</span>, <span class="s1">','</span>, <span class="s1">'how'</span>, <span class="s1">'are'</span>, <span class="s1">'you'</span>, <span class="s1">'doing'</span>, <span class="s1">'today'</span>, <span class="s1">'?'</span><span class="o">]</span>
</code></pre></div></div> <p>Obviously, this approach is not efficient and practical for tokenizing large bodies of text, there are more practical approaches that basically tokenize the text using a <em>vocabulary</em> of possible tokens.</p> <blockquote> <p>“In practice, a compromise between letters and full words is used, and the final vocabulary includes both common words and word fragments from which larger and less frequent words can be composed.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>To have a better understanding of how words can be represented using numbers, let’s take a look at the <em>one-hot encoding</em> procedure, where each word is represented as a sparse vector with a value of 1 in the position corresponding to the word’s index in the vocabulary and 0 elsewhere. Here is a simple Python code to achieve this for a given sentence:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="k">def</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
	    <span class="c1"># Create a dictionary to store one-hot encoded vectors
</span>	    <span class="n">one_hot_vec</span> <span class="o">=</span> <span class="p">{}</span>
	    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
	        <span class="n">one_hot_vec</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span>
	    <span class="c1"># Return the one-hot encoded vector for the given word
</span>	    <span class="k">return</span> <span class="n">one_hot_vec</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
	
	<span class="c1"># Example vocabulary
</span>	<span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">banana</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">grape</span><span class="sh">"</span><span class="p">]</span>
	
	<span class="c1"># Example words to encode
</span>	<span class="n">words_to_encode</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">kiwi</span><span class="sh">"</span><span class="p">]</span>
	
	<span class="c1"># One-hot encode each word
</span>	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_to_encode</span><span class="p">:</span>
	    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
	    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">One-hot vector for </span><span class="sh">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">one_hot_vector</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>returning the following</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	One-hot vector <span class="k">for</span> <span class="s1">'apple'</span>: <span class="o">[</span>1, 0, 0, 0]
	One-hot vector <span class="k">for</span> <span class="s1">'orange'</span>: <span class="o">[</span>0, 0, 1, 0]
	One-hot vector <span class="k">for</span> <span class="s1">'kiwi'</span>: <span class="o">[</span>0, 0, 0, 0]
</code></pre></div></div> <p>Now, let’s go back to the idea of word embedding. This is the process of mapping tokens to numerical vectors in a continuous vector space. This process also helps with semantic relationships and contextual meaning of words and subwords. There are various approaches to achieve this, including</p> <ol> <li><strong>Count-Based Methods</strong> such as <em>TF-IDF</em> and <em>Latent Semantic Analysis</em></li> <li><strong>Prediction-Based Methods</strong> such as <em>Word2Vec</em>, <em>GloVe</em>, and <em>FastText</em></li> <li><strong>Contextual Embeddings</strong> such as <em>ELMo</em>, <em>BERT</em>, <em>GPT</em></li> <li><strong>Subword and Character-Level Embeddings</strong> such as <em>Byte Pair Encoding (BPE)</em>, and <em>Char-CNN</em>, and <em>Char-RNN</em></li> </ol> <p>As an example, let’s take a look at the procedure for <em>Byte Pair Encoding (BPE)</em>, which is a technique that merges commonly occurring sub-strings using the frequency of their occurrence. This process replaces the most frequent pair of bytes (or characters) in a sequence with a single, unused byte (or character). Let’s say we have a set of tokens [“low”, “lowest”, “newer”, “wider”].</p> <ul> <li><em>Step 1</em>: We prepare the input by splitting each word into characters and adding a special end-of-word token <code class="language-plaintext highlighter-rouge">&lt;/w&gt;</code></li> <li><em>Step 2</em>: We create a vocabulary that counts the frequency of each word in the input</li> <li><em>Step 3</em>: We then calculate the frequencies of adjacent character pairs</li> <li><em>Step 4</em>: Let’s say the most frequent pairs are (‘l’, ‘o’). We merge this pair in the vocabulary</li> <li><em>Step 5</em>: We Update the vocabulary and recalculate pair frequencies And we can continue this process in the same manner…</li> </ul> <p>After performing a number of merges, we obtain a vocabulary where common subword units are represented as single tokens. This reduces the vocabulary size and captures subword information that can help with out-of-vocabulary words.</p> <p>Here is a <a href="https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/">python code</a> that you can run to see how the process works:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">re</span>
	<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

	<span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a vocabulary (dictionary mapping words to frequency counts), returns a dictionary of tuples representing the frequency count of pairs of characters
	in the vocabulary.
	</span><span class="sh">"""</span>
	<span class="n">pairs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
		<span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
			<span class="n">pairs</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
	<span class="k">return</span> <span class="n">pairs</span>

	<span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a pair of characters and a vocabulary, returns a new vocabulary with the pair of characters merged together wherever they appear.
	</span><span class="sh">"""</span>
	<span class="n">v_out</span> <span class="o">=</span> <span class="p">{}</span>
	<span class="n">bigram</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">escape</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
	<span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">(?&lt;!\S)</span><span class="sh">'</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="sh">'</span><span class="s">(?!\S)</span><span class="sh">'</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">v_in</span><span class="p">:</span>
		<span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">pair</span><span class="p">),</span> <span class="n">word</span><span class="p">)</span>
		<span class="n">v_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">v_out</span>
  
	<span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a list of strings, returns a dictionary of words mapping to their frequency count in the data.
	</span><span class="sh">"""</span>
	<span class="n">vocab</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
		<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">():</span>
			<span class="n">vocab</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">word</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> &lt;/w&gt;</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
	<span class="k">return</span> <span class="n">vocab</span>

	<span class="k">def</span> <span class="nf">byte_pair_encoding</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a list of strings and an integer n, returns a list of n merged pairs
	of characters found in the vocabulary of the input data.
	</span><span class="sh">"""</span>
	<span class="n">vocab</span> <span class="o">=</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
		<span class="n">pairs</span> <span class="o">=</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
		<span class="n">best</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-----</span><span class="sh">"</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">most frequent pair:</span><span class="sh">'</span><span class="p">,</span> <span class="n">best</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">vocab before merge:</span><span class="sh">'</span><span class="p">,</span><span class="o">*</span><span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
		<span class="n">vocab</span> <span class="o">=</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">vocab after merge:</span><span class="sh">'</span><span class="p">,</span> <span class="o">*</span><span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
	<span class="k">return</span> <span class="n">vocab</span>

<span class="c1"># Example usage:
</span><span class="n">corpus</span> <span class="o">=</span> <span class="sh">"</span><span class="s">low,lowest,newer,wider</span><span class="sh">"</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">)</span>

<span class="n">num_merges</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># set the number of merging steps
</span><span class="n">bpe_pairs</span> <span class="o">=</span> <span class="nf">byte_pair_encoding</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">bpe_pairs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
	<span class="nf">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</code></pre></div></div> <p>once run, this will return</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	-----
	most frequent pair: ('l', 'o')
	vocab before merge: ('l o w &lt;/w&gt;', 1) ('l o w e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	vocab after merge: ('lo w &lt;/w&gt;', 1) ('lo w e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	-----
	most frequent pair: ('lo', 'w')
	vocab before merge: ('lo w &lt;/w&gt;', 1) ('lo w e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	vocab after merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	-----
	most frequent pair: ('e', 'r')
	vocab before merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	vocab after merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er &lt;/w&gt;', 1) ('w i d er &lt;/w&gt;', 1)
	-----
	most frequent pair: ('er', '&lt;/w&gt;')
	vocab before merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er &lt;/w&gt;', 1) ('w i d er &lt;/w&gt;', 1)
	vocab after merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er&lt;/w&gt;', 1) ('w i d er&lt;/w&gt;', 1)
	-----
	most frequent pair: ('low', '&lt;/w&gt;')
	vocab before merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er&lt;/w&gt;', 1) ('w i d er&lt;/w&gt;', 1)
	vocab after merge: ('low&lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er&lt;/w&gt;', 1) ('w i d er&lt;/w&gt;', 1)
	-- Final vocab:
	low&lt;/w&gt; 1
	low e s t &lt;/w&gt; 1
	n e w er&lt;/w&gt; 1
	w i d er&lt;/w&gt; 1
</code></pre></div></div> <p>Once you have your vocabulary vector from an operation like <em>BPE</em>, you then use that to <strong>map</strong> your tokens for an input text to some index.</p> <p>For example, let’s say you end up having a vocabulary with the following index values for the tokens:</p> <table> <thead> <tr> <th>Token</th> <th>The</th> <th>cat</th> <th>sat</th> <th>on</th> <th>the</th> <th>mat</th> </tr> </thead> <tbody> <tr> <td>Index</td> <td>0</td> <td>1</td> <td>2</td> <td>3</td> <td>4</td> <td>5</td> </tr> </tbody> </table> <p>so for an input text “The cat sat” with tokenization [“The”, “cat”, “sat”], you ended up having a token-to-index mapping [0,1,2].</p> <p>Let’s assume that your embedding matrix, \(E\), is a matrix where each row corresponds to the embedding vector of a token in the vocabulary. Let’s assume the embeddings are 3-dimensional, i.e. I can show each embedding with a vector with three components in the vector space. I am also going to assume some random values for the entries. Keep in mind that the embedding matrix \(E\) is typically learned during the training of the model.</p> \[E=\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; -&gt; (\text{embedding for ''The''})\\ 0.4 &amp; 0.5 &amp; 0.6 &amp; -&gt; (\text{embedding for ''cat''})\\ 0.7 &amp; 0.8 &amp; 0.9 &amp; -&gt; (\text{embedding for ''sat''})\\ 0.1 &amp; 0.3 &amp; 0.5 &amp; -&gt; (\text{embedding for ''on''})\\ 0.2 &amp; 0.4 &amp; 0.6 &amp; -&gt; (\text{embedding for ''the''})\\ 0.3 &amp; 0.5 &amp; 0.7 &amp; -&gt; (\text{embedding for ''mat''})\\ \end{bmatrix}\] <p>then, we can look up the corresponding rows in the embedding matrix \(E\) given the indices [0,1,2] for the input matrix (\(X\)). Therefore, our input matrix for the model becomes:</p> \[X=\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 \\ 0.4 &amp; 0.5 &amp; 0.6 \\ 0.7 &amp; 0.8 &amp; 0.9 \\ \end{bmatrix}\] <p>Hopefully this makes the idea of tokenization, embedding matrix, and their relationship to the input sequence very clear. Let’s talk about queries, keys, and values next.</p> <h3 id="queries-keys-and-values-">Queries, Keys, and Values <a name="qkv"></a></h3> <p><strong>Quesries</strong> are vectors that represent the word (or token) for which we want to compute the <em>attention score</em>. Essentially, they are the questions we are asking about the importance of other words. <strong>Keys</strong> are vectors that represent the words in the context we are considering. They act like an index or a reference. <strong>Values</strong> are the actual word embeddings or features that we use to generate the output. They contain the information we are interested in.</p> <p>So at this point, knowing the tokens, we can implement some form of linear transformation with some given weights in the transformation matrices (which they need to be calculated during the training process) to find the queries, keys, and values as follows (Don’t worry too much about what they mean. Juts get a sense of how they can be calculated from input word embeddings. Things become more clear moving forward.)</p> \[Q_i=W_Q\times x_i \quad, \quad K_i=W_K\times x_i \quad , \quad V_i=W_V\times x_i\] <p>Here, \(x_i\) is the <em>vector space</em> for each token, i.e. each row in Matrix \(X\) (in previous section) and \(W\)’s are <strong>weight matrices that need to be found during the training</strong>. Also,</p> \[Q_i=[q_1, q_2, q_3, ...] \quad , \quad K_i=[k_1, k_2, k_3, ...] \quad, \quad \text{and} \quad V_i=[v_i, v_2, v_3,...]\] <p>where \(q_i\), \(k_i\), and \(v_i\) are each a vector.</p> <p>This matrix representation is basically telling you that you will have one query vector, one key vector, and one value vector for each word embedding vector. In another word, now we have three vector representations for each word embedding vector.</p> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/qkv.png" alt="queries-keys-values" width="900" height="250"/> </div> <h3 id="self-attention-mechanism-">Self-Attention Mechanism <a name="selfatt"></a></h3> <p>This mechanism calculates a score between each query and every key to determine how much <em>weight</em> should be given to each word. This weight is usually computed as a dot product of the query and key vectors. We typically apply a softmax to the outcome to keep things under control!</p> \[\text{Attention Weights}=Softmax[K^TQ]\] <p>The weighted values are then calculated using</p> \[\text{Weighted Values}=V.Softmax[K^TQ]\] <p>Let me try to put everything together so you can get a better picture of the general procedure for the case of an input with three tokens:</p> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/attention.png" alt="attention-mechanism" width="700" height="250"/> </div> <p>So at this point, the output for each token incorporates information from all other tokens in the sequence, weighted by their relevance. This means that the representation of token \(i\) is not just based on the token itself but it is also influenced by how much attention it gives to other tokens.</p> <p><strong>NOTE:</strong> In order to deal with large magnitudes in the dot product operation, when calculating the weights, we typically scale the dot product as \(\text{Weighted Values}=V.Softmax[\frac{K^TQ}{\sqrt{D_q}}]\), where \(D_q\) is the dimension of the queries. This scaling procedure when dealing with large magnitudes in attention computation is important since</p> <blockquote> <p>“Small changes to the inputs to the softmax function might have little effect on the output (i.e. the gradients are very small), making the model difficult to train”. ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>We can also write a simple Python code for this procedure and test it out:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
	
	<span class="n">d_k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Dimension of queries and keys
</span>	<span class="n">d_v</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Dimension of values
</span>	
	<span class="c1"># Example word representations (for simplicity, these are random)
</span>	<span class="n">queries</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># 3 words, d_k dimensions
</span>	<span class="n">keys</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>    <span class="c1"># 3 words, d_k dimensions
</span>	<span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>   <span class="c1"># 3 words, d_v dimensions
</span>	
	<span class="c1"># Calculating the dot product between queries and keys (transpose keys)
</span>	<span class="n">dot_products</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dot Products:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">dot_products</span><span class="p">)</span>
	
	<span class="c1"># Applying softmax to get attention weights
</span>	<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	
	<span class="n">attention_weights</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">dot_products</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention Weights:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">)</span>
	
	<span class="c1"># Multiplying the attention weights with the values
</span>	<span class="n">weighted_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">weighted_values</span><span class="p">)</span>

</code></pre></div></div> <p>returning</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	Attention Weights:
	 [[0.46831053 0.06337894 0.46831053]
	 [0.4223188  0.4223188  0.1553624 ]
	 [0.57611688 0.21194156 0.21194156]]
	 
	Weighted Values:
	 [[4.         5.         6.        ]
	 [3.19913082 4.19913082 5.19913082]
	 [2.90747402 3.90747402 4.90747402]]
</code></pre></div></div> <p>If things are still not clear, take a look at the <a href="#struc-example">Attention Mechanism: Learn by Example</a> section, where I went through this procedure with an example calculations for a few tokens in a given sentence.</p> <h3 id="normalization">Normalization<a name="norm"></a></h3> <p>This mechanism, like <em>Batch Normalization</em>, is commonly used in neural networks to stabilize and accelerate the training process. The idea is to normalize the input of each layer so that it has a mean of zero and a variance of one. This helps in mitigating the internal covariate shift problem. Here is how Batch Normalization works:</p> <ol> <li><strong>Calculate Mean and Variance</strong>: For a given batch, calculate the mean and variance of the inputs.</li> <li><strong>Normalize</strong>: Subtract the mean and divide by the standard deviation to normalize the inputs.</li> <li><strong>Scale and Shift</strong>: Apply learned scaling and shifting parameters to the normalized inputs.</li> </ol> <h3 id="positional-encoding">Positional Encoding<a name="posenc"></a></h3> <p>Positional encoding provides information about the position of each word in the sequence. This is essential because unlike recurrent or convolutional layers, transformers do not have a built-in notion of sequence order.</p> <p>Positional encodings are added to the input embeddings to give the model some information about the relative or absolute position of the tokens. The encoding matrix can be created by hand or can be learned. It can be added to the network inputs or at every network layer.</p> <p>One common approach is to use sine and cosine functions of different frequencies. The idea is to generate unique positional encodings that the model can learn to interpret.</p> <p>For even indices:</p> \[\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\] <p>For the odd indices:</p> \[\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\] <p>where</p> <ul> <li>\(pos\) is the position in the sequence,</li> <li>\(i\) is the dimension,</li> <li>\(d_{model}\)​ is the dimension of the model, i.e. size of the vector space in which the tokens are represented.</li> </ul> <p>Here’s a simple implementation of positional encoding in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
	<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
	<span class="n">scaling_factor</span> <span class="o">=</span> <span class="mi">10000</span>
	
	<span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
	    <span class="n">pos_enc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
	    <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
	        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
	            <span class="n">pos_enc</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="n">scaling_factor</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
	            <span class="n">pos_enc</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="n">scaling_factor</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
	    <span class="k">return</span> <span class="n">pos_enc</span>
	
	<span class="c1"># Example usage
</span>	<span class="n">max_len</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Maximum length of the sequence
</span>	<span class="n">d_model</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1"># Dimension of the model
</span>	
	<span class="n">pos_encoding</span> <span class="o">=</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Positional Encoding Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">pos_encoding</span><span class="p">)</span>
	
	<span class="c1"># Visualize the positional encoding
</span>	<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Depth</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Position</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Positional Encoding</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/pencoding.png" alt="positional-encoding" width="500" height="300"/> </div> <p>In the output figure, you can see that for any given position across the dimensions, we have a unique set of color combination.</p> <h2 id="3-attention-mechanism-learn-by-example--">3. Attention Mechanism: Learn by Example <a name="struc-example"></a></h2> <p>In this section, I will try to focus on detailed calculations for the attention mechanism, hoping that it might help those who are more comfortable with actually seeing sample calculations when it comes to understanding a concept. Let’s use the following sentence:</p> <blockquote> <p>Despite the heavy rain, the children played happily in the park, unaware of the approaching storm.</p> </blockquote> <h3 id="tokenization">Tokenization</h3> <p>First, let’s tokenize the sentence into individual words or subwords:</p> <p>Tokens=[ “Despite”, “the” , “heavy” , “rain”, “,” , “the” , “children” , “played” , “happily” , “in” , “the” , “park” , “,” , “unaware” , “of” , “the” , “approaching” , “storm” , “.” ]</p> <h3 id="embedding">Embedding</h3> <p>We then map the tokens into a vector in a high-dimensional space. This mapping is achieved using a pre-trained embedding matrix (e.g., Word2Vec, GloVe, or embeddings learned as part of a transformer model like BERT or GPT).</p> <p>Let’s use a simplified example where each token is mapped to a 4-dimensional vector. In practice, these vectors would be of higher dimensions (e.g., 300, 768, 1024).</p> <p>Here’s an example of what this might look like (with randomly chosen values for illustration):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Despite"→[0.2,−0.1,0.5,0.3]
"the"→[0.1,0.0,−0.1,0.4]
"heavy"→[−0.3,0.8,0.1,0.2]
"rain"→[0.4,0.3,−0.2,0.1]
","→[0.0,0.0,0.0,0.0]
"children"→[0.5,0.2,0.6,−0.1]
"played"→[0.3,0.1,0.4,0.7]
"happily"→[−0.2,0.5,−0.3,0.4]
"in"→[0.1,−0.3,0.2,0.5]
"park"→[0.4,0.6,0.1,−0.4]
"unaware"→[0.2,0.7,−0.5,0.1]
"of"→[0.1,0.0,0.3,−0.2]
"approaching"→[0.3,0.4,0.6,0.2]
"storm"→[0.5,−0.1,0.4,0.3]
"."→[0.0,0.0,0.0,0.0]
</code></pre></div></div> <p>These vectors (embeddings) are typically learned from large corpora of text and capture semantic meanings. For example, “rain” and “storm” might have similar embeddings because they both relate to weather.</p> <h3 id="building-the-embedding-matrix">Building the Embedding Matrix</h3> <p>For our sentence, we create an embedding matrix where each row corresponds to the embedding of a token. If our sentence has 19 tokens and each token is embedded in a 4-dimensional space, our embedding matrix $E$ would be of size \(19\times4\):</p> <table> <thead> <tr> <th>Token</th> <th>Dimension 1</th> <th>Dimension 2</th> <th>Dimension 3</th> <th>Dimension 4</th> </tr> </thead> <tbody> <tr> <td>Despite</td> <td>0.2</td> <td>-0.1</td> <td>0.5</td> <td>0.3</td> </tr> <tr> <td>the</td> <td>0.1</td> <td>0.0</td> <td>-0.1</td> <td>0.4</td> </tr> <tr> <td>heavy</td> <td>-0.3</td> <td>0.8</td> <td>0.1</td> <td>0.2</td> </tr> <tr> <td>rain</td> <td>0.4</td> <td>0.3</td> <td>-0.2</td> <td>0.1</td> </tr> <tr> <td>,</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> </tr> <tr> <td>children</td> <td>0.5</td> <td>0.2</td> <td>0.6</td> <td>-0.1</td> </tr> <tr> <td>played</td> <td>0.3</td> <td>0.1</td> <td>0.4</td> <td>0.7</td> </tr> <tr> <td>happily</td> <td>-0.2</td> <td>0.5</td> <td>-0.3</td> <td>0.4</td> </tr> <tr> <td>in</td> <td>0.1</td> <td>-0.3</td> <td>0.2</td> <td>0.5</td> </tr> <tr> <td>the</td> <td>0.1</td> <td>0.0</td> <td>-0.1</td> <td>0.4</td> </tr> <tr> <td>park</td> <td>0.4</td> <td>0.6</td> <td>0.1</td> <td>-0.4</td> </tr> <tr> <td>unaware</td> <td>0.2</td> <td>0.7</td> <td>-0.5</td> <td>0.1</td> </tr> <tr> <td>of</td> <td>0.1</td> <td>0.0</td> <td>0.3</td> <td>-0.2</td> </tr> <tr> <td>the</td> <td>0.1</td> <td>0.0</td> <td>-0.1</td> <td>0.4</td> </tr> <tr> <td>approaching</td> <td>0.3</td> <td>0.4</td> <td>0.6</td> <td>0.2</td> </tr> <tr> <td>storm</td> <td>0.5</td> <td>-0.1</td> <td>0.4</td> <td>0.3</td> </tr> <tr> <td>.</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> </tr> </tbody> </table> <h3 id="queries-keys-and-values">Queries, Keys, and Values</h3> <p>For illustration purposes, let’s define our weight matrices with arbitrary values</p> \[W_Q=\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4\\ 0.5 &amp; 0.6 &amp; 0.7 &amp; 0.8\\ 0.9 &amp; 1.0 &amp; 1.1 &amp; 1.2\\ 1.3 &amp; 1.4 &amp; 1.5 &amp; 1.6 \end{bmatrix}\] \[W_K=\begin{bmatrix} 1.6 &amp; 1.5 &amp; 1.4 &amp; 1.3\\ 1.2 &amp; 1.1 &amp; 1.0 &amp; 0.9\\ 0.8 &amp; 0.7 &amp; 0.6 &amp; 0.5\\ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \end{bmatrix}\] \[W_V=\begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; -0.4\\ -0.5 &amp; 0.6 &amp; -0.7 &amp; 0.8\\ 0.9 &amp; -1.0 &amp; 1.1 &amp; -1.2\\ -1.3 &amp; 1.4 &amp; -1.5 &amp; 1.6 \end{bmatrix}\] <p>and use them to calculate queries, keys, and values for the first three tokens</p> <ul> <li>“Despite” with embedding [0.2, −0.1, 0.5, 0.3]:</li> </ul> \[Q_{Despite}=W_Q\begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4\\ 0.5 &amp; 0.6 &amp; 0.7 &amp; 0.8\\ 0.9 &amp; 1.0 &amp; 1.1 &amp; 1.2\\ 1.3 &amp; 1.4 &amp; 1.5 &amp; 1.6 \end{bmatrix} \quad\times\quad \begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.27 \\ 0.63 \\ 0.99 \\ 1.35\end{bmatrix}\] \[K_{Despite}=W_K\begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 1.6 &amp; 1.5 &amp; 1.4 &amp; 1.3\\ 1.2 &amp; 1.1 &amp; 1.0 &amp; 0.9\\ 0.8 &amp; 0.7 &amp; 0.6 &amp; 0.5\\ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \end{bmatrix} \quad\times\quad \begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 1.26 \\ 0.9 \\ 0.54 \\ 0.18\end{bmatrix}\] \[V_{Despite}=W_V\begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; -0.4\\ -0.5 &amp; 0.6 &amp; -0.7 &amp; 0.8\\ 0.9 &amp; -1.0 &amp; 1.1 &amp; -1.2\\ -1.3 &amp; 1.4 &amp; -1.5 &amp; 1.6 \end{bmatrix} \quad\times\quad \begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.07 \\ -0.27 \\ 0.47 \\ -0.67\end{bmatrix}\] <ul> <li>“the” with embedding [0.1, 0.0, −0.1, 0.4] and “heavy” with embedding [−0.3, 0.8, 0.1, 0.2]:</li> </ul> \[Q_{the}=W_Q\begin{bmatrix} 0.1 \\ 0.0 \\ -0.1 \\ 0.4 \end{bmatrix}= \begin{bmatrix} 0.14 \\ 0.3 \\ 0.46 \\0.62\end{bmatrix} \quad, \quad Q_{heavy}=W_Q\begin{bmatrix} -0.3 \\ 0.8 \\ 0.1 \\ 0.2 \end{bmatrix}= \begin{bmatrix} 0.24 \\ 0.56 \\ 0.88 \\ 1.2 \end{bmatrix}\] \[K_{the}=W_K\begin{bmatrix} 0.1 \\ 0.0 \\ -0.1 \\ 0.4 \end{bmatrix}= \begin{bmatrix} 0.54 \\ 0.38 \\ 0.22 \\ 0.06\end{bmatrix} \quad, \quad K_{heavy}=W_K\begin{bmatrix} -0.3 \\ 0.8 \\ 0.1 \\ 0.2 \end{bmatrix}= \begin{bmatrix} 1.12 \\ 0.8 \\ 0.48 \\ 0.16\end{bmatrix}\] \[V_{the}=W_V\begin{bmatrix} 0.1 \\ 0.0 \\ -0.1 \\ 0.4 \end{bmatrix}= \begin{bmatrix} -0.18 \\ 0.34 \\ -0.5 \\ 0.66\end{bmatrix} \quad, \quad V_{heavy}=W_V\begin{bmatrix} -0.3 \\ 0.8 \\ 0.1 \\ 0.2 \end{bmatrix}= \begin{bmatrix} -0.24 \\ 0.72 \\ -1.2 \\ 1.68\end{bmatrix}\] <p>By applying the learned weight matrices $W_Q$​, $W_K$​, and $W_V$, we transform the original embeddings into queries, keys, and values. These vectors are then used in the attention mechanism to compute attention scores and to derive contextually rich representations of each token. Let’s see how this works.</p> <h3 id="computing-attention-weights">Computing Attention Weights</h3> <p>For each token, we compute the dot product of its query vector with the key vectors of all tokens. This results in the attention scores for each token relative to every other token.</p> <p>Let’s continue with the example of “Despite” with</p> \[Q_{Despite}=\begin{bmatrix} 0.27 \\ 0.63 \\ 0.99 \\ 1.35\end{bmatrix}\] <p>and then find the attention scores between “Despite” and three other tokens, including itself (“the”, “heavy”, and “Despite” ) with</p> \[K_{the}=\begin{bmatrix} 0.54 \\ 0.38 \\ 0.22 \\ 0.06\end{bmatrix} \quad , \quad K_{heavy}=\begin{bmatrix} 1.12 \\ 0.8 \\ 0.48 \\ 0.16\end{bmatrix} \quad , \quad K_{Despite}=\begin{bmatrix} 1.26 \\ 0.9 \\ 0.54 \\ 0.18\end{bmatrix}\] <ol> <li>Dot product between “Despite” and “the”: \(Dot(Q_{Despite}, K_{the})= 0.1458 + 0.2394 + 0.2178 + 0.081 = 0.684\)</li> <li>Dot product between “Despite” and “heavy”: \(Dot(Q_{Despite}, K_{heavy})= 0.3024 + 0.504 + 0.4752 + 0.216 = 1.4976\)</li> <li>Dot product between “Despite” and “Despite” (self-attention): \(Dot(Q_{Despite}, K_{Despite})= 0.3402 + 0.567 + 0.5346 + 0.243 = 1.6848\)</li> </ol> <p>These dot products represent the raw attention weights for “Despite” in relation to “the”, “heavy,” and itself.</p> \[\text{Softmax}([0.684, 1.4976, 1.6848]) = \left[ \frac{e^{0.684}}{e^{0.684} + e^{1.4976} + e^{1.6848}}, \frac{e^{1.4976}}{e^{0.684} + e^{1.4976} + e^{1.6848}}, \frac{e^{1.6848}}{e^{0.684} + e^{1.4976} + e^{1.6848}} \right]\] <p>These softmax values represent the normalized attention scores for “Despite” with respect to “the,” “heavy,” and itself, respectively. They indicate the relative importance of “Despite” compared to the other tokens during the attention mechanism.</p> <h2 id="4-transformer-models-">4. Transformer Models <a name="models"></a></h2> <p>We can generally classify transformers into three models, <strong>Encoders</strong>, <strong>Decoders</strong>, and <strong>Encoder-Decoders</strong>. An encoder:</p> <blockquote> <p>“…transforms the text embeddings into a representation that can support variety of tasks.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>A decoder, however, is typically used to generate the next output and to continue the given input text, like GPT models.</p> <p>Finally, encoder-decoders are implemented in</p> <blockquote> <p>“…sequence-to-sequence tasks, where one text string is converted into another.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>A common example for encoder-decoder model is language translation.</p> <h2 id="5-coding-a-simple-transformer-">5. Coding a Simple Transformer <a name="struc"></a></h2> <p>In this section, we will try to develop a simple transformer piece-by-piece using Python. This simple model includes an embedding layer, positional encoding, attention mechanism, a feed-forward neural network, normalization layer, as well as encoder and decoder parts. Keep in mind that a typical Transformer has a much more complex structure with a extra components (such as residual connections and <a href="https://aiml.com/explain-self-attention-and-masked-self-attention-as-used-in-transformers/">masked attention</a>) and it also is implemented through a <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html"><em>multi-headed</em></a> approach for parallel computation. Here, <strong>the goal is to have a better understanding of how the building blocks of a Transformer piece together.</strong></p> <h3 id="embedding-layer">Embedding Layer</h3> <p>Let’s start with the embedding layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">EmbeddingLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding_matrix</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>

</code></pre></div></div> <p>Here, we use a simple example, the sentence “I love learning from examples” with a given vocabulary, and see how this embedding layer works:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining the vocabulary and input sentence
</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">I</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">learning</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">from</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">examples</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="n">input_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">I</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">learning</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Converting the input sentence to indices
</span><span class="n">input_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_sentence</span><span class="p">]</span>

<span class="c1"># Initializing the embedding layer
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Let's use 4 dimensions for simplicity
</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="nc">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="c1"># Getting the embeddings for the input indices
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="nf">embedding_layer</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">input_indices</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Indices:</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embeddings:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre></div></div> <p>The above code will give the following output (yours will be different since I used <code class="language-plaintext highlighter-rouge">np.random.rand</code> in the <code class="language-plaintext highlighter-rouge">EmbeddingLayer</code> class so every run gives a different set of outputs):</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Indices: <span class="o">[</span>0, 1, 2]

Embeddings:
 <span class="o">[[</span>0.90034368 0.82241612 0.02018069 0.62033932]
 <span class="o">[</span>0.6351551  0.33107626 0.78112305 0.21081683]
 <span class="o">[</span>0.93584476 0.08042298 0.05619254 0.74456291]]
</code></pre></div></div> <h3 id="positional-encoding-1">Positional Encoding</h3> <p>We continue our model development by creating a positional encoding layer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

</code></pre></div></div> <p>Here is the implementation of the class using the example sentence in the previous section:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pos_encoding_layer</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> 
<span class="n">embeddings_with_positional_encoding</span> <span class="o">=</span> <span class="nf">pos_encoding_layer</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:])</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embeddings with Positional Encoding:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">embeddings_with_positional_encoding</span><span class="p">)</span>
</code></pre></div></div> <p>which will return (yours will be different since I used <code class="language-plaintext highlighter-rouge">np.random.rand</code> in the <code class="language-plaintext highlighter-rouge">EmbeddingLayer</code> class so every run gives a different set of outputs)</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Embeddings with Positional Encoding:
 <span class="o">[[[</span> 0.90034368  1.82241612  0.02018069  1.62033932]
  <span class="o">[</span> 1.47662609  0.87137857  0.79112288  1.21076683]
  <span class="o">[</span> 1.84514219 <span class="nt">-0</span>.33572386  0.07619121  1.74436292]]]
</code></pre></div></div> <p>In the output, the positional encoding are those same vectors as embeddings with positional information added. The positional encoding modifies the embeddings to incorporate the position of each word in the sentence, making it easier for the model to learn the order of words.</p> <h3 id="self-attention-mechanism">Self-Attention Mechanism</h3> <p>Once we have our embeddings, we need to have our self attention layer, which can be coded as follows</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_q</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_k</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_v</span><span class="p">)</span>
        
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_o</span><span class="p">)</span>
</code></pre></div></div> <p>Again, let us look at how it operates using a simple example, assuming a sentence with 3 tokens, for a 4-dimensional model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A simple input matrix (3 words with 4-dimensional embeddings)
</span><span class="n">input_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Dimensionality of the input
</span><span class="n">self_attention_layer</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">output_matrix</span> <span class="o">=</span> <span class="nf">self_attention_layer</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Matrix after Self-Attention:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">)</span>
</code></pre></div></div> <p>The above input example will return (yours will be different since I used <code class="language-plaintext highlighter-rouge">np.random.rand</code> in the <code class="language-plaintext highlighter-rouge">EmbeddingLayer</code> class so every run gives a different set of outputs)</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Matrix:
 <span class="o">[[</span>1 0 1 0]
 <span class="o">[</span>0 1 0 1]
 <span class="o">[</span>1 1 1 1]]
 
Output Matrix after Self-Attention:
 <span class="o">[[</span>4.00438965 3.73163309 4.01682177 3.50892256]
 <span class="o">[</span>4.08366923 3.80766097 4.10031155 3.5832559 <span class="o">]</span>
 <span class="o">[</span>4.37889227 4.08744366 4.40506536 3.85251859]]
</code></pre></div></div> <p>In the output, the input matrix represents three words with four features each. And the output matrix shows the result of applying the self-attention mechanism to the input matrix, essentially showing how each word’s representation is influenced by the others in the sequence.</p> <p>We need two more layers, one fully-connected feed-forward neural network and one normalization layer.</p> <h3 id="feed-forward-neural-network">Feed-Forward Neural Network</h3> <p>This layer is just a typical fully connected neural network. The purpose of the feed-forward network is to transform the attention output.</p> <p>The ReLU activation introduces non-linearity, allowing the model to learn more complex patterns and representations beyond what linear transformations can capture. Further, this layer can help capture richer features and improve the model’s ability to generalize. Finally, this layer allows the model to process each token’s representation separately after attending to other tokens, enhancing the model’s capacity to learn individual token features.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
	    <span class="c1"># d_model: dimension of the input and output (same as the input embedding dimension).
</span>	    <span class="c1"># d_ff: dimension of the hidden layer in the feedforward network.
</span>	    
		<span class="c1"># A weight matrix of shape `(d_model, d_ff)` initialized with random values.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
		<span class="c1"># A weight matrix of shape `(d_ff, d_model)` initialized with random values.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
	    <span class="c1"># `np.maximum(0, np.dot(x, self.W1))`: applies the ReLU activation function to introduce non-linearity.
</span>	    <span class="c1"># `np.dot(np.maximum(0, np.dot(x, self.W1)), self.W2)`: projects the hidden layer back to the original dimension `d_model`.
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W1</span><span class="p">)),</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">)</span>

</code></pre></div></div> <h3 id="layer-normalization">Layer Normalization</h3> <p>The <code class="language-plaintext highlighter-rouge">LayerNorm</code> class implements layer normalization. The layer normalization is applied to the inputs of the sub-layers in the Transformer architecture to stabilize and accelerate the training process. In the transformer architecture, layer normalization is applied at various points, typically before and after the main sub-layers (self-attention and feed-forward neural network).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>

</code></pre></div></div> <h3 id="encoder-layer">Encoder Layer</h3> <p>Now, putting all these components together, we can define a simple <strong>Encoder</strong> structure as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div></div> <p>And here is an example code to test the procedure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple example input matrix (3 words with 4-dimensional embeddings) 
</span><span class="n">input_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]])</span> 

<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Dimension of the input 
</span><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># Dimension of the hidden layer in the feedforward network 
</span><span class="n">encoder_layer</span> <span class="o">=</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span> 

<span class="c1"># Applying the EncoderLayer to the input matrix 
</span><span class="n">output_matrix</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">)</span> 

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_matrix</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Matrix after EncoderLayer:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">)</span>
</code></pre></div></div> <p>The final output matrix (<code class="language-plaintext highlighter-rouge">output_matrix</code>) contains the transformed embeddings of the input tokens. Each row in this matrix represents a token in the sequence, but now these token representations are context-aware and enriched with information from the entire sequence.</p> <p>The potential applications of the output are</p> <ul> <li><strong>Context-Awareness</strong>: Each token’s representation in the output matrix is affected by other tokens in the sequence, capturing some of the dependencies and relationships.</li> <li><strong>Foundation for Further Processing</strong>: This output serves as the input for subsequent layers in the Transformer model, building progressively richer representations that can be used for tasks like translation, classification, or other sequence-to-sequence tasks.</li> </ul> <h3 id="decoder-layer">Decoder Layer</h3> <p>We can also construct a <strong>Decoder</strong> using the concepts and codes we previously introduced:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>
        <span class="c1"># Self-attention on the decoder's own input
</span>        <span class="n">self_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Cross-attention with the encoder's output
</span>        <span class="n">enc_dec_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">enc_dec_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">enc_dec_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Feedforward network
</span>        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div></div> <p>Here, there is a new concept called <strong>Cross-Attention</strong> (or encoder-decoder attention).</p> <p>Remember that a Transformer pays attention to various tokens in a given sentence using self-attention mechanism. However, when we have a decoder, paying attention to the tokens in a sentence in the decoder is not enough, we also need to pay attention to the encoder’s outputs. For instance in a language translation procedure, when we couple an encoder with a decoder, the model needs to pay attention to the words in the translated sentence in the output language (self-attention) <strong>and</strong> the words in the original language (cross-attention). Therefore, using this cross-attention layer, decoder embeddings attend to the encoder embeddings. In this scenario,</p> <blockquote> <p>“… the queries are computed from the decoder embeddings and the keys and values from the encoder embeddings.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>Let us then take a look at a simple example of a decoder along with an example of how it takes a set of inputs and returns an output which is a final transformed matrix, capturing the relationships between the decoder’s input tokens and the encoder’s output tokens. This matrix is then can be passed to the next layer of the Transformer decoder or to be used for generating the output sequence.</p> <p>Putting it all together with an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_q</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_k</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_v</span><span class="p">)</span>
        
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_o</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W1</span><span class="p">)),</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>

<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>
        <span class="c1"># Self-attention on the decoder's own input
</span>        <span class="n">self_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Cross-attention with the encoder's output
</span>        <span class="n">enc_dec_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">enc_dec_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">enc_dec_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Feedforward network
</span>        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Example input matrix (decoder's input)
</span><span class="n">decoder_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> 
                          <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> 
                          <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]])</span>

<span class="c1"># Example encoder output matrix
</span><span class="n">encoder_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> 
                           <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> 
                           <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">]])</span>

<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Dimensionality of the input
</span><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">8</span>     <span class="c1"># Dimensionality of the hidden layer in the feedforward network
</span><span class="n">decoder_layer</span> <span class="o">=</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

<span class="n">output_matrix</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoder Input Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Encoder Output Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Matrix after DecoderLayer:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">)</span>
</code></pre></div></div> <h2 id="6-final-comments-">6. Final Comments <a name="fincom"></a></h2> <p>The main place to experiment with various Transformer-based models, used for NLP, Computer Vision, and automatic speech recognition is the <a href="https://huggingface.co/docs/transformers/en/index">Hugging Face</a>. I strongly suggest joining the community and learn by following tutorials and implementing models for your own projects.</p> <p>To have a much better understanding of how the mechanics of Transformers for NLP-related tasks work along with learning how to implement various NLP-related tasks, I suggest looking at <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Andrej Karpathy YouTube channel</a>, specially <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=8">Let’s build GPT: from scratch, in code, spelled out.</a> and <a href="https://www.youtube.com/watch?v=l8pRSuU81PU&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=11">Let’s reproduce GPT-2 (124M)</a>.</p> <p>If you are looking for more comprehensive and clear discussion about Transformers and their working principles, I suggest <a href="https://issuu.com/cmb321/docs/deep_learning_ebook">Deep Learning: Foundations and Concepts</a> by <em>Christopher M. Bishop</em> and <em>Hugh Bishop</em> (chapter 12) and also <a href="https://github.com/udlbook/udlbook/releases/download/v4.0.1/UnderstandingDeepLearning_05_27_24_C.pdf">Understanding Deep Learning</a> by <em>Simon J.D. Prince</em> (chapter 12).</p>]]></content><author><name></name></author><category term="technical"/><category term="transformers,"/><category term="python,"/><category term="NLP"/><summary type="html"><![CDATA[A clear explanation of working principles of Transformers for NLP tasks]]></summary></entry><entry><title type="html">Data-Driven vs. First-Principles</title><link href="https://masoudmim.github.io/blog/2024/problem-solving/" rel="alternate" type="text/html" title="Data-Driven vs. First-Principles"/><published>2024-03-30T10:18:00+00:00</published><updated>2024-03-30T10:18:00+00:00</updated><id>https://masoudmim.github.io/blog/2024/problem-solving</id><content type="html" xml:base="https://masoudmim.github.io/blog/2024/problem-solving/"><![CDATA[<figure style="text-align: center; max-width: 500px; margin: 0 auto;"> <audio controls="" style="width: 100%;"> <source src="/assets/audio/data-driven.mp3" type="audio/mpeg"/> Your browser does not support the audio element. </audio> <figcaption>(Listen to this article)</figcaption> </figure> <p><br/></p> <div style="text-align: justify;"> <p>In the realm of problem-solving methodologies, two distinct yet powerful approaches emerge: the first-principles method and the data-driven approach. The first-principles strategy involves dissecting problems into fundamental truths and building solutions from there, grounded in foundational concepts, scientific laws, and logical reasoning. Conversely, the data-driven approach leverages available or collected data to extract insights and patterns, relying on statistical analysis, machine learning, and data mining for valuable information. These methodologies play pivotal roles in innovation and problem-solving across diverse fields.</p> <p>In first-principles approach, a deep understanding of system components and their workings is essential. For instance, in engineering, implementing laws like Newton's laws of motion or conservation of energy allows for a detailed study of physical systems. Similarly, domain-specific knowledge plays a crucial role, such as a contractor estimating renovation costs or an engineer sizing radiators for a space. Examples of problems effectively tackled using the first-principles approach include trajectory calculations for projectiles, structural designs like bridges, optimizing mechanical systems, predicting chemical reactions, designing materials, and developing simplified mathematical models for complex systems like population dynamics or economic models.</p> <div style="display: flex; justify-content: center; align-items: center; gap: 20px; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/first-principles.jpg" alt="first-principle" width="400" height="350"/> <img src="/assets/img/data-driven.jpg" alt="data-driven" width="400" height="350"/> </div> <p>Contrastingly, the data-driven approach utilizes available or collected data to extract insights, patterns, and potential solutions. It heavily relies on statistical analysis, machine learning, and data mining to derive valuable information from large datasets. This method is particularly useful for complex systems where understanding is limited, such as predicting weather or financial trends. Examples of effective data-driven problem-solving include sales forecasting, customer behavior prediction, and stock price estimation through machine learning. In healthcare, data analysis aids in disease diagnosis and predicting patient outcomes. Businesses leverage data-driven models for targeted advertising, personalized recommendations, and customer segmentation. Among the most significant challenges in adopting the data-driven approach is the difficulty of distinguishing <a href="https://en.wikipedia.org/wiki/The_Signal_and_the_Noise">noise from the signal</a>. Additionally, it lacks the capability to predict rare events, also known as <a href="https://www.youtube.com/watch?v=t7Fr6iGhmBM">fat tail scenarios</a>.</p> <p>While these approaches differ, they are not mutually exclusive. Combining them can lead to a robust problem-solving strategy. The first-principles approach starts from fundamental principles, while the data-driven approach begins with existing data, ultimately enhancing adaptability and flexibility. Data-driven techniques are efficient for studying complex systems, while the first-principles approach is less biased and more adaptable to new situations. In real-world scenarios, a hybrid approach often yields optimal results. Known as physics-informed machine learning, it combines physical models with data-driven analytics. For instance, in weather forecasting, combining atmospheric dynamics models with machine learning improves accuracy. Similarly, in drug discovery, integrating molecular simulations with machine learning predicts drug efficacy faster. Moreover, energy system optimization benefits from physics-based models and data analytics, while supply chain management improves through combining mathematical models with data-driven insights. </p> <p>By embracing both first-principles and data-driven approaches, we navigate complex problems more effectively, ensuring solutions that are both theoretically sound and empirically validated.</p> </div>]]></content><author><name></name></author><category term="essay"/><category term="problem-solving,"/><category term="data-driven"/><summary type="html"><![CDATA[First-Principles Meets Data-Driven Problem Solving]]></summary></entry><entry><title type="html">Unmasking Educational Illusions: A Journey Towards Antifragility</title><link href="https://masoudmim.github.io/blog/2024/unmasking-educational-illusion/" rel="alternate" type="text/html" title="Unmasking Educational Illusions: A Journey Towards Antifragility"/><published>2024-03-18T19:45:00+00:00</published><updated>2024-03-18T19:45:00+00:00</updated><id>https://masoudmim.github.io/blog/2024/unmasking-educational-illusion</id><content type="html" xml:base="https://masoudmim.github.io/blog/2024/unmasking-educational-illusion/"><![CDATA[<figure style="text-align: center; max-width: 500px; margin: 0 auto;"> <audio controls="" style="width: 100%;"> <source src="/assets/audio/unmasking-education.mp3" type="audio/mpeg"/> Your browser does not support the audio element. </audio> <figcaption>(Listen to this article)</figcaption> </figure> <p><br/></p> <div style="text-align: justify;"> <p>The exploration of <a href="https://en.wikipedia.org/wiki/Antifragility#:~:text=Antifragility%20is%20a%20property%20of,Antifragile%2C%20and%20in%20technical%20papers">Antifragility</a>, as illuminated by <a href="https://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb">Nassim Taleb</a>, can spark profound inquiries into the foundational principles of contemporary education. It is vital to acknowledge that while Taleb's critiques of the educational system are enlightening, the following discussion represents personal reflections and interpretations.</p> <p>Central to Taleb's discourse is the provocative assertion regarding the perceived utility of a college degree:</p> <blockquote> <p>"If a U.S. college degree appears to be useless, it is by design. For 'liberal arts' = training for upper-class free men (liber) who were above having a profession. You learn to earn \$$ in vocational or professional schools. Middle-class parents were tricked &amp; fleeced."</p> </blockquote> <p>This perspective unveils a pervasive ambiguity surrounding the objectives and outcomes of modern education. Many students embark on their college journey with the expectation of securing lucrative careers that validate their investment. However, a stark misalignment often exists between these aspirations and the educational system's capacity to fulfill them.</p> <p>The prevalent narrative advocates for a comprehensive college education encompassing vocational skills and intellectual growth. Yet, this broad mandate has inadvertently led to disillusionment among students, parents, and educators alike. Students envision tangible returns on their educational investments, anticipating heightened employability. However, the reality often diverges drastically, leaving numerous graduates grappling with underemployment and unfulfilled expectations. This disconnect has cultivated a culture of credentialism, resulting in students facing mounting debt burdens, and the intrinsic value of learning is overshadowed by credential-centric pursuits.</p> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/technology-is-the-result-of-antifragility-exploit.jpg" alt="technology is the result of antifragility exploit.jpg" width="400" height="250"/> </div> <p>Taleb's insight, encapsulated in the quote, <blockquote> <p>"We have the illusion that the world functions thanks to programmed design, university research, and bureaucratic funding, but there is very compelling evidence to show that this is an illusion, the illusion I call 'lecturing birds how to fly,'"</p> </blockquote> challenges the entrenched notions of educational efficacy. It underscores the pivotal role of antifragility in fostering technological progress—an attribute nurtured through risk-taking, experimentation, and resilience, rather than rigid structures and bureaucratic oversight. This perspective reframes the purpose of education, advocating for a curriculum that prioritizes adaptability, creativity, and critical thinking. It prompts a critical examination of the balance between theoretical knowledge and practical skills, urging educators to cultivate problem-solving abilities and innovation among students.</p> <p>The traditional higher education model, centered on rote memorization and standardized assessments, inadequately equips students for the dynamic challenges of the contemporary workforce. Institutions advertising to prepare students with required skills for the job market must prioritize agility, innovation, and real-world relevance in their educational frameworks. This necessitates a departure from standardized learning experiences towards personalized, experiential learning opportunities bolstered by robust industry collaborations.</p> <p>I, however, am not specifically advocating for this shift in views and expectations. I think there needs to be a much clearer discussion about what we expect from a college education. Do we want to have citizens who are cultured and are familiar with art, science, and history? Or are we looking to train skilled workers? I also fully agree with Taleb that the role of university research in technology development has been greatly exaggerated, sometimes to justify budgetary requests for research. That is not to say the research has not resulted in advances in technology; it is to emphasize that historically, we have seen the majority of innovations and advances in technology coming not from theorizing but from tinkerers with a great sense of curiosity and risk-taking attitude. Here is what Taleb says:</p> <blockquote> <p>"Technology is the result of antifragility, exploited by risk-takers in the form of tinkering and trial and error, with nerd-driven design confined to the backstage. Engineers and tinkerers develop things while history books are written by academics; we will have to refine historical interpretations of growth, innovation, and many such things."</p> </blockquote> <p>Embracing Taleb's concept of antifragility in education necessitates a transformative shift—a departure from the conventional <a href="https://twitter.com/black_swan_man/status/1439932806424109056/photo/1">'lecturing birds how to fly'</a> approach towards an ethos that celebrates exploration, adaptability, and continuous learning. Educators must evolve into facilitators of curiosity and discovery, guiding students towards becoming resilient agents of change and innovation. Although this is much easier said than done, especially within the constraints of the current educational system and the dynamic between families, college administration, and educators, which imposes extreme limitations on what can and needs to be accomplished.</p> </div>]]></content><author><name></name></author><category term="essay"/><category term="education,"/><category term="antifragility"/><summary type="html"><![CDATA[Exploring Antifragility and Its Implications for Education]]></summary></entry><entry><title type="html">Engineering the Unknown: A Startup Voyage</title><link href="https://masoudmim.github.io/blog/2024/engineering-the-unknown/" rel="alternate" type="text/html" title="Engineering the Unknown: A Startup Voyage"/><published>2024-03-03T20:15:00+00:00</published><updated>2024-03-03T20:15:00+00:00</updated><id>https://masoudmim.github.io/blog/2024/engineering-the-unknown</id><content type="html" xml:base="https://masoudmim.github.io/blog/2024/engineering-the-unknown/"><![CDATA[<blockquote> <p><strong>Author’s Note:</strong> The essay you’re about to read was initially written on May 3, 2017, summarizing my personal journey as an engineer navigating the startup landscape. This updated version seeks to improve the narrative, offering further insights and underscoring the lasting significance of the experiences shared.</p> </blockquote> <figure style="text-align: center; max-width: 500px; margin: 0 auto;"> <audio controls="" style="width: 100%;"> <source src="/assets/audio/engineering-the-unknown.mp3" type="audio/mpeg"/> Your browser does not support the audio element. </audio> <figcaption>(Listen to this article)</figcaption> </figure> <p><br/></p> <div style="text-align: justify;"> <p>Embracing the role of an engineer has always been a source of immense joy for me, with its inherent challenges in design and manufacturing that fuel my fascination for the intricacies of my work and research. Each day is an exploration, filled with learning, modeling, experimenting, and designing, where every task demands creativity and innovation to prevent monotony from overshadowing the excitement of the job.</p> <p>Yet, my perspective underwent a transformative shift when I transitioned into the dynamic landscape of startup companies. Initially, the overwhelming cloud of uncertainty left me bewildered and apprehensive. The pace of change was dizzying, and the pressure to meet intense deadlines was relentless. Delivering results became a precarious endeavor, given the constant fluctuations in the company's mission, prompted by shifts in market demands or cash burn rates. Surprisingly, this chaotic environment provided an adrenaline rush that forever altered my perception of working in a conventional engineering firm. The startup experience propelled me into a new frontier, introducing me to a world beyond the confines of routine engineering.</p> <div style="text-align: center;"> <img src="/assets/img/Plato-cave.png" alt="plato’s allegory of the cave" width="400" height="250"/> </div> <p>Engaging with innovators, investors, individuals from hedge fund companies, and daring entrepreneurs unveiled the intricacies of transforming a brilliant idea into a successful, sustainable business. The journey, akin to Plato's allegory of the cave, mirrored my transition from being chained to a fixed position, solely focused on the wall in front of me, to an unshackled explorer. This newfound perspective, reminiscent of Neo's awakening in the Matrix, was made possible by the founder of the startup—the Morpheus who offered me the red pill by providing the opportunity to work in such an environment.</p> <p>Despite the demanding nature of startup life for an engineer—its difficulties, demands, and arduous challenges—I definitely advocate this experience for every aspiring engineer. It serves as a vessel, weaving the intricate tale of how an idea, against all odds, metamorphoses into a product. It narrates the story of survival through the tumultuous initial years, battling various problems, and eventually giving birth to a company that may boast thousands of employees. This experience is not one that can be easily described; it is an odyssey that must be lived, an invaluable opportunity for personal and professional growth that I fervently recommend to young engineers, if the chance arises.</p> </div>]]></content><author><name></name></author><category term="essay"/><category term="startup-life"/><summary type="html"><![CDATA[Thoughts on Working in a Startup Company]]></summary></entry></feed>