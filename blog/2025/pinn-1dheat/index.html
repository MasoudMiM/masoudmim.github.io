<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="physics-informed-neural-network-for-1d-heat-transfer">Physics Informed Neural Network for 1D Heat Transfer</h1> <p><a href="https://en.wikipedia.org/wiki/Physics-informed_neural_networks" rel="external nofollow noopener" target="_blank">Physics-Informed Neural Networks</a> (PINNs) are a promising way to blend deep learning with physical laws. This typically allows us to solve differential equations without needing traditional numerical discretization methods. In this post, I look at a simple example to show how PINN can be used to solve an engineering problem, one-dimensional heat transfer.</p> <p>Although the one-dimensional heat equation is a trivial problem when compared with more complex engineering scenarios, it can serve as an excellent starting point to understand the mechanics behind PINNs. These ideas can be extended to solve multidimensional and highly nonlinear engineering problems.</p> <p>Before we look at implementing PINN to solve 1D heat transfer problem, I first look at how we can calculate the numerical derivatives using auto-differentiation in <a href="https://www.tensorflow.org/" rel="external nofollow noopener" target="_blank">TensorFlow</a>.</p> <h2 id="auto-differentiation-in-tensorflow">Auto-Differentiation in TensorFlow</h2> <p>One of the key features of PINNs is the use of automatic differentiation. TensorFlow’s “GradientTape” records operations on tensors to compute derivatives. Consider the simple polynomial function:</p> \[f(x) = x^3 + 2x^2 + 3x + 4\] <p>Its analytical derivative is:</p> <p>  \(f'(x) = 3x^2 + 4x + 3\)</p> <p>Below is a code snippet that shows how to get the derivative using TensorFlow:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># This is the original function
</span><span class="k">def</span> <span class="nf">polynomial_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">4</span>

<span class="c1"># Here, we create a TensorFlow variable named x_tf with an initial value of 2.0. This represents the input value for which we want to compute the derivative.
</span><span class="n">x_tf</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>  

<span class="c1"># This is how we keep a record of operating on tensors that involve differentiable computations. TensorFlow will later use this recorded information to compute gradients.
</span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span> 
    <span class="c1"># This line computes the value of the desired function (in this case, a polynomial) using the variable x_tf as input.
</span>    <span class="n">y_tf</span> <span class="o">=</span> <span class="nf">polynomial_function</span><span class="p">(</span><span class="n">x_tf</span><span class="p">)</span>

<span class="c1"># Now that we have the derivative recorded within "tape", we can compute the derivative of y_tf with respect to x_tf.
</span><span class="n">numerical_derivative</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">y_tf</span><span class="p">,</span> <span class="n">x_tf</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Numerical derivative at x=2.0:</span><span class="sh">"</span><span class="p">,</span> <span class="n">numerical_derivative</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
</code></pre></div></div> <p>In this snippet, GradientTape “watches” the variable x_tf. Every operation on x_tf is recorded so that TensorFlow can automatically compute the derivative. You can see the complete code for comparing the analytical derivative and the one calculated using this approach at <a href="https://github.com/MasoudMiM/pinn-1d-heat/blob/main/derivative_example.py" rel="external nofollow noopener" target="_blank">this address</a>. The output for this function is as follows:</p> <p><br></p> <div style="text-align: center;"> <img src="/assets/img/derivative_example.png" alt="derivative using gradient" width="500"> </div> <p><br></p> <p>This same mechanism isused when computing derivatives with respect to spatial and temporal variables in the PINN example later.</p> <p>Now, with this in mind, let’s look at the example of one-dimensional heat transfer problem with given boundary and initial conditions.</p> <h2 id="pinns-for-1d-heat-transfer">PINNs for 1D Heat Transfer</h2> <p>The 1D heat equation is given by:</p> <p>  \(u_t = \alpha \ u_{xx}\)</p> <p>where:</p> <ul> <li>\(u(x, t)\) is the temperature distribution.</li> <li>\(u_t\) is the time derivative,</li> <li>\(u_{xx}\) is the second spatial derivative, and</li> <li>\(\alpha\) is the thermal diffusivity.</li> </ul> <p>In our PINN approach, the neural network learns a mapping from (x, t) to u(x, t) such that the network output satisfies the heat equation, initial condition, and boundary conditions.</p> <h3 id="neural-network-structure">Neural Network Structure</h3> <p>The network is designed to take two inputs (x and t) and produces one output (u), usually through a series of fully connected layers. A common architecture might be:</p> <ul> <li>Input layer of size 2 for (x, t).</li> <li>Two or more hidden layers with multiple neurons each and an activation function.</li> <li>One output neuron representing the predicted temperature.</li> </ul> <p>Below is a representative snippet for defining the neural network:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)),</span>               <span class="c1"># (x, t) input
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">),</span>    <span class="c1"># first hidden layer
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">),</span>    <span class="c1"># second hidden layer
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                         <span class="c1"># output layer: u(x, t)
</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">build</span><span class="p">((</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># Initializing weights by building the model
</span></code></pre></div></div> <h3 id="embedding-the-physics">Embedding the Physics</h3> <p>Instead of training solely on data, a PINN embeds the differential equation into the loss function. We compute the derivatives u_t and u_xx using TensorFlow’s GradientTape and define a residual that must be minimized:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="c1"># Here, we combine inputs for the forward pass
</span>    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">tape</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">tape</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># predicting temperature
</span>        
        <span class="c1"># Then, we compute first derivatives with respect to x and t
</span>        <span class="n">U_t</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">U_x</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="c1"># and second derivative with respect to x
</span>        <span class="n">U_xx</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">U_x</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    
    <span class="c1"># We then define the residual as u_t - alpha*u_xx, which should be close to zero based on the governing equation
</span>    <span class="n">residual</span> <span class="o">=</span> <span class="n">U_t</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">U_xx</span>
    
    <span class="k">del</span> <span class="n">tape</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">residual</span><span class="p">))</span>
</code></pre></div></div> <p>Notice that we are only computing the residual from the heat equation. Next, we enforce the boundary and initial conditions.</p> <h3 id="incorporating-boundary-and-initial-conditions">Incorporating Boundary and Initial Conditions</h3> <p>In the 1D heat problem, the boundary conditions might be prescribed, for example, here we consider \(u(0, t) = 0\) and \(u(L, t) = 0\), which are Dirichlet conditions for the rod’s ends. Similarly, the initial condition can also be defined as \(u(x, 0) = sin(\pi x)\), which is a common test case with a known analytical solution.</p> <p>I am now going to add their contributions to the overall loss function as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">boundary_conditions_loss</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">u_left</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">t</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># u(0,t)=0
</span>    <span class="n">u_right</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">L</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">t</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># u(L,t)=0
</span>    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">u_left</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">u_right</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">initial_conditions_loss</span><span class="p">(</span><span class="n">N_x</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
    <span class="n">x_initial</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">convert_to_tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">N_x</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">t_initial</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x_initial</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">u_initial_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">x_initial</span><span class="p">,</span> <span class="n">t_initial</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">u_initial_true</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_initial</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">u_initial_pred</span> <span class="o">-</span> <span class="n">u_initial_true</span><span class="p">))</span>
</code></pre></div></div> <h3 id="complete-loss-function">Complete Loss Function</h3> <p>The complete loss would sum the PDE residual loss, the boundary condition loss, and the initial condition loss as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">tape</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">tape</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">U_t</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">U_x</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">U_xx</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">U_x</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="n">residual</span> <span class="o">=</span> <span class="n">U_t</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">U_xx</span>
    <span class="n">bc_loss</span> <span class="o">=</span> <span class="nf">boundary_conditions_loss</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">ic_loss</span> <span class="o">=</span> <span class="nf">initial_conditions_loss</span><span class="p">(</span><span class="n">N_x</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>

    <span class="c1"># This is our complete loss function, including initial condition, boundary condition, and residual loss (representing the physics of the problem)
</span>    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">residual</span><span class="p">))</span> <span class="o">+</span> <span class="n">bc_loss</span> <span class="o">+</span> <span class="n">ic_loss</span>

    <span class="k">del</span> <span class="n">tape</span>
    
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></div> <p>Training adjusts the network’s weights to minimize the combined loss. This optimization process produces a model that simulates heat transfer accurately compared to the analytical solution.</p> <h3 id="training-the-pinn">Training the PINN</h3> <p>The network is optimized using an algorithm like <a href="https://keras.io/api/optimizers/adam/" rel="external nofollow noopener" target="_blank">Adam</a>, which makes small adjustments to the weights based on the computed loss values. A simplified training loop might look like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">X_flat_tf</span><span class="p">,</span> <span class="n">t_flat_tf</span><span class="p">)</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="n">loss_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The loop above iteratively improves the neural network so that the output function \(u(x, t)\) satisfies the heat equation as well as the imposed initial and boundary conditions. Here is a comparison between the analytical solution and the output from the trained PINN.</p> <p><br></p> <div style="text-align: center;"> <img src="/assets/img/1D_heat_pinn_20250531_023932.png" alt="1D heat transfer using PINN" width="500"> </div> <p><br></p> <p>For a full implementation of the complete PINN for 1D heat transfer, check out <a href="https://github.com/MasoudMiM/pinn-1d-heat/blob/main/thermal_diffusion.py" rel="external nofollow noopener" target="_blank">this link</a>.</p> <h2 id="why-this-example-matters">Why This Example Matters</h2> <p>This 1D heat transfer example is trivial in the sense that the analytical solution is known. However, the same principles apply to far more complex problems in engineering mechanics, including:</p> <ul> <li>Multidimensional heat transfer in irregular geometries.</li> <li>Fluid-structure interactions where the governing equations are highly nonlinear.</li> <li>Coupled phenomena in material science or biomechanics.</li> </ul> <p>For such problems, traditional numerical methods (e.g., finite element or finite difference) might struggle with mesh generation, handling complex boundaries, and high-dimensional parameter spaces. PINNs, in contrast, handle these issues by embedding the physics directly into the neural network’s loss function, offering a mesh-free and flexible computational framework.</p> </body></html>