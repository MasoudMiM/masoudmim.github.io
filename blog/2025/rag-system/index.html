<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Building a RAG System from Text Documents with Milvus and Ollama | Masoud Masoumi</title> <meta name="author" content="Masoud Masoumi"> <meta name="description" content="Masoud Masoumi's personal website "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://masoudmim.github.io/blog/2025/rag-system/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Masoud Masoumi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Building a RAG System from Text Documents with Milvus and Ollama</h1> <p class="post-meta">June 16, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rag"> <i class="fa-solid fa-hashtag fa-sm"></i> RAG,</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM,</a>   <a href="/blog/tag/ollama"> <i class="fa-solid fa-hashtag fa-sm"></i> Ollama,</a>   <a href="/blog/tag/milvus"> <i class="fa-solid fa-hashtag fa-sm"></i> Milvus</a>     ·   <a href="/blog/category/technical"> <i class="fa-solid fa-tag fa-sm"></i> technical</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Who read documentaions for a software or instructions for a system anymore, right?! <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation" rel="external nofollow noopener" target="_blank">Retrieval-Augmented Generation</a>, or RAG is one of the most practical applications of AI for working with large text collections. Instead of hoping a language model knows about your specific domain, RAG lets you feed it exactly the information it needs to give better answers.</p> <p>The beauty of RAG lies in its simplicity. Rather than fine-tuning massive models or hoping your documents were in the training data, you create a searchable knowledge base from your texts and retrieve relevant pieces when someone asks a question. Think of it as giving your AI assistant a really good filing system and the ability to quickly find the right documents before answering.</p> <p>Here, we’ll build a complete RAG system using Milvus as our vector database and Ollama for running language models locally. We’ll work through the entire process from a raw text, which is the <a href="https://gmsh.info/doc/texinfo/gmsh.html" rel="external nofollow noopener" target="_blank">documentaion for the Gmsh software</a> to a web interface where users can ask questions and get contextual answers.</p> <h2 id="understanding-the-rag-architecture">Understanding the RAG Architecture</h2> <p>We will develop two main procedures: the indexing phase where we process documents and store them, and the retrieval phase where we answer questions.</p> <h3 id="indexing">Indexing</h3> <p>we take a text document and break it into meaningful chunks. These chunks get converted into <a href="https://www.pinecone.io/learn/vector-embeddings/" rel="external nofollow noopener" target="_blank">vector embeddings</a> using a sentence transformer model. These embeddings capture the semantic meaning of the text in a way that allows us to find similar content later. We store both the embeddings and the original text in Milvus, which serves as our vector database.</p> <p>When someone asks a question, we convert their query into the same type of embedding, search for the most similar chunks in our database, and then feed both the question and the relevant context to a language model. The LLM generates an answer based on the retrieved information rather than just its training data.</p> <pre><code class="language-mermaid">graph TB
    A[Text Document] --&gt; B[Text Processing &amp; Chunking]
    B --&gt; C[Sentence Transformer]
    C --&gt; D[Vector Embeddings]
    D --&gt; E[Milvus Vector Database]
    
    F[User Query] --&gt; G[Query Embedding]
    G --&gt; H[Similarity Search in Milvus]
    H --&gt; I[Retrieve Top-K Documents]
    I --&gt; J[Context + Query]
    J --&gt; K[Ollama LLM]
    K --&gt; L[Generated Answer]
</code></pre> <h2 id="setting-up-the-tools">Setting Up the Tools</h2> <p>We’ll need <a href="https://milvus.io/" rel="external nofollow noopener" target="_blank">Milvus</a> running as our vector database, <a href="https://ollama.com/" rel="external nofollow noopener" target="_blank">Ollama</a> with a language model for generation, and several Python libraries for text processing and embeddings. Milvus handles the heavy lifting of similarity search across high-dimensional vectors, while Ollama gives us local access to language models without sending data to external APIs.</p> <p>For the embedding model, we’re using <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">all-MiniLM-L6-v2</code></a> from Sentence Transformers. This model strikes a good balance between quality and speed, producing 384-dimensional vectors that capture semantic meaning effectively. It’s particularly good at understanding the relationships between different pieces of text, which is exactly what we need for retrieval.</p> <h2 id="processing-documents-into-searchable-chunks">Processing Documents into Searchable Chunks</h2> <p>The key to good RAG performance is how you chunk your documents. Too small and you lose context, too large and the retrieval becomes unfocused. I will try grouping 5 sentences together in this example, hoping that it provides enough context while keeping chunks focused on specific topics.</p> <p>Here’s how we handle the document processing:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_vector_database</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">'</span><span class="s">all-MiniLM-L6-v2</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># This will break text into sentences and group them
</span>    <span class="n">sentences</span> <span class="o">=</span> <span class="nf">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">text_chunks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">])</span> 
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">)</span>
    <span class="p">]</span>
    
    <span class="c1"># This converts chunks to embeddings
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text_chunks</span><span class="p">)</span>
    <span class="n">embeddings_list</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
    
    <span class="c1"># This will store the data into a Milvus collection
</span>    <span class="nf">connect_to_milvus</span><span class="p">()</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="nf">create_milvus_collection</span><span class="p">(</span><span class="n">MILVUS_COLLEC_NAME</span><span class="p">)</span>
    <span class="nf">insert_vectors_to_milvus</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">embeddings_list</span><span class="p">,</span> <span class="n">text_chunks</span><span class="p">)</span>
</code></pre></div></div> <p>The sentence tokenization ensures we break text at natural boundaries, while the chunking strategy maintains coherent thoughts. Each chunk gets converted to a 384-dimensional vector that represents its semantic content.</p> <h2 id="building-the-milvus-collection">Building the Milvus Collection</h2> <p>To do this, I basically follow the same process that I discussed in a <a href="2025-04-13-text-to-vector-with-milvus.md">previous post</a>. Milvus requires us to define a schema for our data before we can start storing vectors. Our schema is straightforward: we need an ID field for each record, the embedding vector itself, and the original text. The embedding field uses Milvus’s <code class="language-plaintext highlighter-rouge">FLOAT_VECTOR</code> type with 384 dimensions to match our sentence transformer output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_milvus_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">):</span>
    <span class="n">fields</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">INT64</span><span class="p">,</span> <span class="n">is_primary</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">auto_id</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">FLOAT_VECTOR</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">384</span><span class="p">),</span>
        <span class="nc">FieldSchema</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DataType</span><span class="p">.</span><span class="n">VARCHAR</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
    <span class="p">]</span>
    
    <span class="n">schema</span> <span class="o">=</span> <span class="nc">CollectionSchema</span><span class="p">(</span><span class="n">fields</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Vector database for documentation.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="nc">Collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">collection</span>
</code></pre></div></div> <p>After creating the collection and inserting our data, we build an index to make searches fast. The <code class="language-plaintext highlighter-rouge">IVF_FLAT</code> index type works well for most use cases. We also load the collection into memory so searches happen quickly.</p> <p><br></p> <div style="text-align: center;"> <img src="/assets/img/gmsh_collection.png" alt="milvus collection created from gmsh documentation" width="700"> </div> <p><br></p> <h2 id="implementing-the-question-answering-logic">Implementing the Question-Answering Logic</h2> <p>The real magic happens during query time. When someone asks a question, we need to find the most relevant chunks from our database and use them to generate an answer. This involves converting the question to an embedding, searching for similar chunks, and then crafting a prompt that gives the language model both the question and the relevant context.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">retrieve_similar_documents</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># This will convert query into embedding usng the same model
</span>    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">query</span><span class="p">]).</span><span class="nf">tolist</span><span class="p">()</span>
    
    <span class="c1"># Then, we search for similar chunks in the Milvus collection we created
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">collection</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">query_embedding</span><span class="p">,</span>
        <span class="n">anns_field</span><span class="o">=</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">param</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">metric_type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">L2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">nprobe</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">}},</span>
        <span class="n">limit</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">output_fields</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>
    
    <span class="c1"># We then extract the tex and similarity scores
</span>    <span class="n">retrieved_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">hits</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">hit</span> <span class="ow">in</span> <span class="n">hits</span><span class="p">:</span>
            <span class="n">retrieved_docs</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
                <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">hit</span><span class="p">.</span><span class="n">entity</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">:</span> <span class="n">hit</span><span class="p">.</span><span class="n">score</span>
            <span class="p">})</span>
    
    <span class="k">return</span> <span class="n">retrieved_docs</span>
</code></pre></div></div> <p>The similarity search uses <a href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="external nofollow noopener" target="_blank">L2 distance</a> to find chunks whose embeddings are closest to the query embedding. Lower scores indicate higher similarity, and we focus on retrieving the top 5 matches to provide sufficient context without overwhelming the language model.</p> <h2 id="crafting-effective-prompts">Crafting Effective Prompts</h2> <p>Once we have relevant chunks, we need to present them to the language model in a way that encourages accurate, contextual answers. The prompt structure is crucial here. We provide clear instructions, include all the retrieved context, and then ask the specific question.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_rag_prompt</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">retrieved_docs</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">retrieved_docs</span><span class="p">])</span>
    
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">You are a helpful assistant that answers questions based on the provided context. Use the context below to answer the user</span><span class="sh">'</span><span class="s">s question. If the answer cannot be found in the context, say so clearly.

    Context:
    </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">

    Question: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">

    Answer:</span><span class="sh">"""</span>
    
    <span class="k">return</span> <span class="n">prompt</span>
</code></pre></div></div> <p>This prompt structure helps the model understand its role and encourages it to stick to the provided context rather than hallucinating information. The clear separation between context and question makes it easy for the model to understand what information is available and what’s being asked.</p> <h2 id="connecting-to-ollama-for-local-generation">Connecting to Ollama for Local Generation</h2> <p>Using Ollama for generation keeps everything local and gives us control over the model and its parameters. We send our crafted prompt to Ollama’s API and get back a generated response. The temperature and other parameters can be tuned based on whether you want more creative or more focused answers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query_ollama</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">OLLAMA_BASE_URL</span><span class="si">}</span><span class="s">/api/generate</span><span class="sh">"</span>
    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="n">OLLAMA_MODEL</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">stream</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">options</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">top_p</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">top_k</span><span class="sh">"</span><span class="p">:</span> <span class="mi">40</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">)</span>
</code></pre></div></div> <p>The streaming option is set to false because we want to get the complete response at once for our web interface. In a production system, you might want to enable streaming for better user experience with longer answers.</p> <h2 id="building-the-web-interface">Building the Web Interface</h2> <p>The final piece is creating a user-friendly interface where people can ask questions and see both the answers and the source material. <a href="https://streamlit.io/" rel="external nofollow noopener" target="_blank">Streamlit</a> makes this straightforward with its simple component model and built-in state management.</p> <p>The interface shows system status to help users understand if everything is connected properly, provides a text area for questions, and displays both the generated answer and the source chunks that were used. This transparency is important because users can see where the information came from and judge the reliability of the answer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">st</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">RAG System - Documentation Assistant</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">rag</span> <span class="o">=</span> <span class="nf">initialize_rag</span><span class="p">()</span>
    <span class="n">milvus_status</span> <span class="o">=</span> <span class="n">rag</span><span class="p">.</span><span class="nf">connect_to_milvus</span><span class="p">()</span>
    
    <span class="n">question</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="nf">text_area</span><span class="p">(</span><span class="sh">"</span><span class="s">Enter your question:</span><span class="sh">"</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">st</span><span class="p">.</span><span class="nf">button</span><span class="p">(</span><span class="sh">"</span><span class="s">Get Answer</span><span class="sh">"</span><span class="p">)</span> <span class="ow">and</span> <span class="n">question</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="ow">and</span> <span class="n">milvus_status</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">st</span><span class="p">.</span><span class="nf">spinner</span><span class="p">(</span><span class="sh">"</span><span class="s">Searching for relevant information...</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">answer</span><span class="p">,</span> <span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">rag</span><span class="p">.</span><span class="nf">ask_question</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
            
            <span class="n">st</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">**Answer:** </span><span class="si">{</span><span class="n">answer</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            
            <span class="k">with</span> <span class="n">st</span><span class="p">.</span><span class="nf">expander</span><span class="p">(</span><span class="sh">"</span><span class="s">Source Documents</span><span class="sh">"</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">retrieved_docs</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">st</span><span class="p">.</span><span class="nf">markdown</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">**Source </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">** (Score: </span><span class="si">{</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
                    <span class="n">st</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">][:</span><span class="mi">300</span><span class="p">]</span> <span class="o">+</span> <span class="sh">"</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The expandable source section lets users dive deeper into where the answer came from without cluttering the main interface. The similarity scores help users understand how confident the system is about the relevance of each source chunk.</p> <p><br></p> <div style="text-align: center;"> <img src="/assets/img/rag-gmsh" alt="streamlit interface for the rag system" width="700"> </div> <p><br></p> <h2 id="how-it-all-works-together">How It All Works Together</h2> <p>When someone asks a question through the web interface, the system follows a clear sequence. First, it converts the question into an embedding using the same sentence transformer model we used for indexing. Then it searches the Milvus database for chunks with similar embeddings. These chunks, along with the original question, get formatted into a prompt that’s sent to Ollama. The language model generates an answer based on the provided context, and the web interface displays both the answer and the source material.</p> <pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant Streamlit
    participant RAGSystem
    participant Milvus
    participant Ollama
    
    User-&gt;&gt;Streamlit: Asks question
    Streamlit-&gt;&gt;RAGSystem: Process query
    RAGSystem-&gt;&gt;RAGSystem: Convert query to embedding
    RAGSystem-&gt;&gt;Milvus: Search similar documents
    Milvus--&gt;&gt;RAGSystem: Return top-k results
    RAGSystem-&gt;&gt;RAGSystem: Create contextualized prompt
    RAGSystem-&gt;&gt;Ollama: Send prompt with context
    Ollama--&gt;&gt;RAGSystem: Generate response
    RAGSystem--&gt;&gt;Streamlit: Return answer + sources
    Streamlit--&gt;&gt;User: Display results
</code></pre> <p>This approach ensures that answers are grounded in your actual documents rather than the model’s training data. It’s particularly powerful for domain-specific knowledge where you need accurate, up-to-date information that might not be widely available.</p> <h2 id="performance-considerations">Performance Considerations</h2> <p>The system is designed to be responsive for typical documentation sizes. Milvus handles the vector similarity search efficiently. The bottleneck is typically the language model generation, which depends on the model size and your hardware.</p> <p>Caching the sentence transformer model in Streamlit prevents reloading on every query, and keeping the Milvus collection loaded in memory ensures fast searches. For larger document collections, you might need to consider more sophisticated indexing strategies or distributed setups.</p> <p>The chunk size and retrieval parameters can be tuned based on your specific use case. Technical documentation might benefit from larger chunks to maintain context, while FAQ-style content might work better with smaller, more focused chunks.</p> <h2 id="real-world-applications">Real-World Applications</h2> <p>This RAG system works well for many practical applications. Technical documentation, like the Gmsh documentation we used as an example, benefits from the system’s ability to find specific procedures and code examples. Internal company knowledge bases can be made searchable and accessible to employees without extensive training.</p> <p>Research papers and academic literature can be processed to create specialized question-answering systems for specific fields. Legal documents, policy manuals, and regulatory texts all work well with this approach because accuracy and source attribution are crucial.</p> <p>The local nature of the system using Ollama makes it suitable for sensitive or proprietary documents where you can’t send data to external APIs. Everything runs on your own infrastructure, giving you complete control over your data.</p> <p>Whether you’re building a customer support tool, research assistant, or internal knowledge system, this RAG implementation gives you the core functionality needed to get started.</p> <p><em>Complete code and setup instructions are available in the GitHub repository <a href="https://github.com/MasoudMiM/rag-example" rel="external nofollow noopener" target="_blank">here</a></em>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/transformers/">Transformers - Fundamental Concepts with Python Implementation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/svd/">Dynamic Mode Decomposition for Fluid Dynamics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/text-to-vector-with-milvus/">Turning Text into a Vector Database with Milvus</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/pinn-1dheat/">A Physics Informed Neural Network for 1D Heat Transfer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/execution/">Ideas Matter More Than Ever</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023-2025 Masoud Masoumi. Modified by Masoud Masoumi based on the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>