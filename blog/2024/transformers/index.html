<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transformers - Fundamental Concepts with Python Implementation | Masoud Masoumi</title> <meta name="author" content="Masoud Masoumi"> <meta name="description" content="A clear explanation of working principles of Transformers for NLP tasks"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://masoudmim.github.io/blog/2024/transformers/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Masoud Masoumi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformers - Fundamental Concepts with Python Implementation</h1> <p class="post-meta">June 14, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers,</a>   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> python,</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>     ·   <a href="/blog/category/technical"> <i class="fa-solid fa-tag fa-sm"></i> technical</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="table-of-contents">Table of Contents</h4> <ol> <li><a href="#motiv">Motivation - Why Transformers?</a></li> <li> <a href="#term">Terminology</a> <ul> <li><a href="#softmax">Softmax</a></li> <li><a href="#tokeniz">Tokenization and Word Embedding</a></li> <li><a href="#qkv">Queries, Keys, and Values</a></li> <li><a href="#selfatt">Self Attention Mechanism</a></li> <li><a href="#norm">Normalization</a></li> <li><a href="#posenc">Positional Encoding</a></li> </ul> </li> <li><a href="#struc-example">Attention Mechanism: Learn by Example</a></li> <li><a href="#models">Transformer Models</a></li> <li><a href="#struc">Coding a Simple Transformer</a></li> <li><a href="#fincom">Final Comments</a></li> </ol> <h2 id="1-motivation---why-transformers-">1. Motivation - Why Transformers? <a name="motiv"></a> </h2> <p>If you are interested in modern natural language processing (NLP), machine learning, and artificial intelligence, you probably need to learn <a href="https://arxiv.org/pdf/1706.03762" rel="external nofollow noopener" target="_blank">Transformers</a> and know how they can be implemented for various tasks. Transformers have set new benchmarks in a variety of NLP tasks such as language translation, text summarization, and question answering. Models like <a href="https://huggingface.co/docs/transformers/en/model_doc/bert" rel="external nofollow noopener" target="_blank">BERT</a>, <a href="https://en.wikipedia.org/wiki/GPT-3" rel="external nofollow noopener" target="_blank">GPT-3</a>, <a href="https://openai.com/index/gpt-4/" rel="external nofollow noopener" target="_blank">GPT-4</a>, <a href="https://huggingface.co/docs/transformers/en/model_doc/t5" rel="external nofollow noopener" target="_blank">T5</a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/llama" rel="external nofollow noopener" target="_blank">LLaMA</a>, and <a href="https://claude.ai/chats" rel="external nofollow noopener" target="_blank">Claude</a> are all implementing Transformers in their architectures. Transformers have also been adapted for use in other domains such as computer vision (<a href="https://huggingface.co/docs/transformers/en/model_doc/vit" rel="external nofollow noopener" target="_blank">Vision Transformers ViTs</a>) and <a href="https://huggingface.co/docs/transformers/en/model_doc/speech_to_text" rel="external nofollow noopener" target="_blank">speech processing</a>. One major advantage of Transformers, when compared with Recurrent Neural Networks, is their capability for parallelization.</p> <p>Understanding Transformers provides a foundation for delving into more advanced topics in AI and machine learning. Many current research directions and innovations build upon the Transformer architecture, making it essential knowledge for staying up-to-date in the field. Further, the popularity of Transformers has led to a wealth of resources, including research papers, tutorials, open-source implementations (like <a href="https://huggingface.co/docs/transformers/en/index" rel="external nofollow noopener" target="_blank">Hugging Face’s Transformers library</a>), and community support. This makes it easier to learn, experiment, and apply Transformer models in various projects.</p> <h2 id="2-terminology-">2. Terminology <a name="term"></a> </h2> <p>Transformers use a mechanism called <strong>attention</strong> to determine the importance of different words in a sequence. The core components of this <strong>attention mechanism</strong> are <em>queries</em>, <em>keys</em>, and <em>values</em>. Let’s take a closer look at some fundamental components of Transformers first and familiarize ourselves with some important concepts.</p> <h3 id="softmax">softmax<a name="softmax"></a> </h3> <p>Softmax function is a mathematical function that converts a vector of values into a probability distribution. It is widely used in machine learning, especially in classification tasks and attention mechanisms, because it transforms input values to be in the range (0, 1) and ensures they sum up to 1. The softmax function is defined as follows:</p> \[\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\] <p>where:</p> <ul> <li>\(x_i\)​ is the \(i^{th}\) element of the input vector.</li> <li>\(e\) is the base of the natural logarithm.</li> </ul> <p>and the denominator is the sum of the exponentials of all elements in the input vector. Here is a simple implementation in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
	
	<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_x</span><span class="p">)</span>
	
	<span class="c1"># Example input vector
</span>	<span class="n">input_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
	
	<span class="c1"># Compute softmax
</span>	<span class="n">softmax_output</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">input_vector</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Vector:</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_vector</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Softmax Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">softmax_output</span><span class="p">)</span>
</code></pre></div></div> <p>which will return</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	Input Vector: <span class="o">[</span>1. 2. 3.]
	Softmax Output: <span class="o">[</span>0.09003057 0.24472847 0.66524096] 
</code></pre></div></div> <h3 id="tokenization-and-word-embedding">Tokenization and Word Embedding<a name="tokeniz"></a> </h3> <p>This is the process of breaking down a text into smaller units, which are called <em>tokens</em>. These tokens can be words, subwords, or characters. Tokenization is an important step in natural language processing because it transforms raw text data into a format that can be processed and converted into numerical format later (using embedding process).</p> <p><em>Word embeddings</em> are dense vector representations of words in a high-dimensional space, where similar words are closer to each other in this space. This is where we convert words into <em>numerical</em> format, which is essential for computing purposes. Here is a simple Python code to get a better sense of tokenization procedure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">re</span>
	
	<span class="k">def</span> <span class="nf">simple_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
	    <span class="c1"># Define a regular expression pattern for tokenization
</span>	    <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">(\b\w+\b|[.,!?;])</span><span class="sh">'</span><span class="p">)</span>
	    <span class="c1"># Use findall to get all matches
</span>	    <span class="n">tokens</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
	    <span class="k">return</span> <span class="n">tokens</span>
	
	<span class="c1"># Example text
</span>	<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Hello, how are you doing today?</span><span class="sh">"</span>
	
	<span class="c1"># Tokenize the text
</span>	<span class="n">tokens</span> <span class="o">=</span> <span class="nf">simple_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
	
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original Text:</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Tokens:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p>with the output</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original Text: Hello, how are you doing today?
Tokens: <span class="o">[</span><span class="s1">'Hello'</span>, <span class="s1">','</span>, <span class="s1">'how'</span>, <span class="s1">'are'</span>, <span class="s1">'you'</span>, <span class="s1">'doing'</span>, <span class="s1">'today'</span>, <span class="s1">'?'</span><span class="o">]</span>
</code></pre></div></div> <p>Obviously, this approach is not efficient and practical for tokenizing large bodies of text, there are more practical approaches that basically tokenize the text using a <em>vocabulary</em> of possible tokens.</p> <blockquote> <p>“In practice, a compromise between letters and full words is used, and the final vocabulary includes both common words and word fragments from which larger and less frequent words can be composed.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>To have a better understanding of how words can be represented using numbers, let’s take a look at the <em>one-hot encoding</em> procedure, where each word is represented as a sparse vector with a value of 1 in the position corresponding to the word’s index in the vocabulary and 0 elsewhere. Here is a simple Python code to achieve this for a given sentence:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="k">def</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
	    <span class="c1"># Create a dictionary to store one-hot encoded vectors
</span>	    <span class="n">one_hot_vec</span> <span class="o">=</span> <span class="p">{}</span>
	    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
	        <span class="n">one_hot_vec</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span>
	    <span class="c1"># Return the one-hot encoded vector for the given word
</span>	    <span class="k">return</span> <span class="n">one_hot_vec</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
	
	<span class="c1"># Example vocabulary
</span>	<span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">banana</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">grape</span><span class="sh">"</span><span class="p">]</span>
	
	<span class="c1"># Example words to encode
</span>	<span class="n">words_to_encode</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">apple</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">kiwi</span><span class="sh">"</span><span class="p">]</span>
	
	<span class="c1"># One-hot encode each word
</span>	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_to_encode</span><span class="p">:</span>
	    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
	    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">One-hot vector for </span><span class="sh">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">one_hot_vector</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>returning the following</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	One-hot vector <span class="k">for</span> <span class="s1">'apple'</span>: <span class="o">[</span>1, 0, 0, 0]
	One-hot vector <span class="k">for</span> <span class="s1">'orange'</span>: <span class="o">[</span>0, 0, 1, 0]
	One-hot vector <span class="k">for</span> <span class="s1">'kiwi'</span>: <span class="o">[</span>0, 0, 0, 0]
</code></pre></div></div> <p>Now, let’s go back to the idea of word embedding. This is the process of mapping tokens to numerical vectors in a continuous vector space. This process also helps with semantic relationships and contextual meaning of words and subwords. There are various approaches to achieve this, including</p> <ol> <li> <strong>Count-Based Methods</strong> such as <em>TF-IDF</em> and <em>Latent Semantic Analysis</em> </li> <li> <strong>Prediction-Based Methods</strong> such as <em>Word2Vec</em>, <em>GloVe</em>, and <em>FastText</em> </li> <li> <strong>Contextual Embeddings</strong> such as <em>ELMo</em>, <em>BERT</em>, <em>GPT</em> </li> <li> <strong>Subword and Character-Level Embeddings</strong> such as <em>Byte Pair Encoding (BPE)</em>, and <em>Char-CNN</em>, and <em>Char-RNN</em> </li> </ol> <p>As an example, let’s take a look at the procedure for <em>Byte Pair Encoding (BPE)</em>, which is a technique that merges commonly occurring sub-strings using the frequency of their occurrence. This process replaces the most frequent pair of bytes (or characters) in a sequence with a single, unused byte (or character). Let’s say we have a set of tokens [“low”, “lowest”, “newer”, “wider”].</p> <ul> <li> <em>Step 1</em>: We prepare the input by splitting each word into characters and adding a special end-of-word token <code class="language-plaintext highlighter-rouge">&lt;/w&gt;</code> </li> <li> <em>Step 2</em>: We create a vocabulary that counts the frequency of each word in the input</li> <li> <em>Step 3</em>: We then calculate the frequencies of adjacent character pairs</li> <li> <em>Step 4</em>: Let’s say the most frequent pairs are (‘l’, ‘o’). We merge this pair in the vocabulary</li> <li> <em>Step 5</em>: We Update the vocabulary and recalculate pair frequencies And we can continue this process in the same manner…</li> </ul> <p>After performing a number of merges, we obtain a vocabulary where common subword units are represented as single tokens. This reduces the vocabulary size and captures subword information that can help with out-of-vocabulary words.</p> <p>Here is a <a href="https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/" rel="external nofollow noopener" target="_blank">python code</a> that you can run to see how the process works:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">re</span>
	<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

	<span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a vocabulary (dictionary mapping words to frequency counts), returns a dictionary of tuples representing the frequency count of pairs of characters
	in the vocabulary.
	</span><span class="sh">"""</span>
	<span class="n">pairs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
		<span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
			<span class="n">pairs</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">freq</span>
	<span class="k">return</span> <span class="n">pairs</span>

	<span class="k">def</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a pair of characters and a vocabulary, returns a new vocabulary with the pair of characters merged together wherever they appear.
	</span><span class="sh">"""</span>
	<span class="n">v_out</span> <span class="o">=</span> <span class="p">{}</span>
	<span class="n">bigram</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">escape</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">pair</span><span class="p">))</span>
	<span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">(?&lt;!\S)</span><span class="sh">'</span> <span class="o">+</span> <span class="n">bigram</span> <span class="o">+</span> <span class="sa">r</span><span class="sh">'</span><span class="s">(?!\S)</span><span class="sh">'</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">v_in</span><span class="p">:</span>
		<span class="n">w_out</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">pair</span><span class="p">),</span> <span class="n">word</span><span class="p">)</span>
		<span class="n">v_out</span><span class="p">[</span><span class="n">w_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_in</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">v_out</span>
  
	<span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a list of strings, returns a dictionary of words mapping to their frequency count in the data.
	</span><span class="sh">"""</span>
	<span class="n">vocab</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
		<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">():</span>
			<span class="n">vocab</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">word</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> &lt;/w&gt;</span><span class="sh">'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
	<span class="k">return</span> <span class="n">vocab</span>

	<span class="k">def</span> <span class="nf">byte_pair_encoding</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
	<span class="sh">"""</span><span class="s">
	Given a list of strings and an integer n, returns a list of n merged pairs
	of characters found in the vocabulary of the input data.
	</span><span class="sh">"""</span>
	<span class="n">vocab</span> <span class="o">=</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
		<span class="n">pairs</span> <span class="o">=</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
		<span class="n">best</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-----</span><span class="sh">"</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">most frequent pair:</span><span class="sh">'</span><span class="p">,</span> <span class="n">best</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">vocab before merge:</span><span class="sh">'</span><span class="p">,</span><span class="o">*</span><span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
		<span class="n">vocab</span> <span class="o">=</span> <span class="nf">merge_vocab</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">vocab after merge:</span><span class="sh">'</span><span class="p">,</span> <span class="o">*</span><span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
	<span class="k">return</span> <span class="n">vocab</span>

<span class="c1"># Example usage:
</span><span class="n">corpus</span> <span class="o">=</span> <span class="sh">"</span><span class="s">low,lowest,newer,wider</span><span class="sh">"</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">)</span>

<span class="n">num_merges</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># set the number of merging steps
</span><span class="n">bpe_pairs</span> <span class="o">=</span> <span class="nf">byte_pair_encoding</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">bpe_pairs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
	<span class="nf">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</code></pre></div></div> <p>once run, this will return</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	-----
	most frequent pair: ('l', 'o')
	vocab before merge: ('l o w &lt;/w&gt;', 1) ('l o w e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	vocab after merge: ('lo w &lt;/w&gt;', 1) ('lo w e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	-----
	most frequent pair: ('lo', 'w')
	vocab before merge: ('lo w &lt;/w&gt;', 1) ('lo w e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	vocab after merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	-----
	most frequent pair: ('e', 'r')
	vocab before merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w e r &lt;/w&gt;', 1) ('w i d e r &lt;/w&gt;', 1)
	vocab after merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er &lt;/w&gt;', 1) ('w i d er &lt;/w&gt;', 1)
	-----
	most frequent pair: ('er', '&lt;/w&gt;')
	vocab before merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er &lt;/w&gt;', 1) ('w i d er &lt;/w&gt;', 1)
	vocab after merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er&lt;/w&gt;', 1) ('w i d er&lt;/w&gt;', 1)
	-----
	most frequent pair: ('low', '&lt;/w&gt;')
	vocab before merge: ('low &lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er&lt;/w&gt;', 1) ('w i d er&lt;/w&gt;', 1)
	vocab after merge: ('low&lt;/w&gt;', 1) ('low e s t &lt;/w&gt;', 1) ('n e w er&lt;/w&gt;', 1) ('w i d er&lt;/w&gt;', 1)
	-- Final vocab:
	low&lt;/w&gt; 1
	low e s t &lt;/w&gt; 1
	n e w er&lt;/w&gt; 1
	w i d er&lt;/w&gt; 1
</code></pre></div></div> <p>Once you have your vocabulary vector from an operation like <em>BPE</em>, you then use that to <strong>map</strong> your tokens for an input text to some index.</p> <p>For example, let’s say you end up having a vocabulary with the following index values for the tokens:</p> <table> <thead> <tr> <th>Token</th> <th>The</th> <th>cat</th> <th>sat</th> <th>on</th> <th>the</th> <th>mat</th> </tr> </thead> <tbody> <tr> <td>Index</td> <td>0</td> <td>1</td> <td>2</td> <td>3</td> <td>4</td> <td>5</td> </tr> </tbody> </table> <p>so for an input text “The cat sat” with tokenization [“The”, “cat”, “sat”], you ended up having a token-to-index mapping [0,1,2].</p> <p>Let’s assume that your embedding matrix, \(E\), is a matrix where each row corresponds to the embedding vector of a token in the vocabulary. Let’s assume the embeddings are 3-dimensional, i.e. I can show each embedding with a vector with three components in the vector space. I am also going to assume some random values for the entries. Keep in mind that the embedding matrix \(E\) is typically learned during the training of the model.</p> \[E=\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; -&gt; (\text{embedding for ''The''})\\ 0.4 &amp; 0.5 &amp; 0.6 &amp; -&gt; (\text{embedding for ''cat''})\\ 0.7 &amp; 0.8 &amp; 0.9 &amp; -&gt; (\text{embedding for ''sat''})\\ 0.1 &amp; 0.3 &amp; 0.5 &amp; -&gt; (\text{embedding for ''on''})\\ 0.2 &amp; 0.4 &amp; 0.6 &amp; -&gt; (\text{embedding for ''the''})\\ 0.3 &amp; 0.5 &amp; 0.7 &amp; -&gt; (\text{embedding for ''mat''})\\ \end{bmatrix}\] <p>then, we can look up the corresponding rows in the embedding matrix \(E\) given the indices [0,1,2] for the input matrix (\(X\)). Therefore, our input matrix for the model becomes:</p> \[X=\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 \\ 0.4 &amp; 0.5 &amp; 0.6 \\ 0.7 &amp; 0.8 &amp; 0.9 \\ \end{bmatrix}\] <p>Hopefully this makes the idea of tokenization, embedding matrix, and their relationship to the input sequence very clear. Let’s talk about queries, keys, and values next.</p> <h3 id="queries-keys-and-values-">Queries, Keys, and Values <a name="qkv"></a> </h3> <p><strong>Quesries</strong> are vectors that represent the word (or token) for which we want to compute the <em>attention score</em>. Essentially, they are the questions we are asking about the importance of other words. <strong>Keys</strong> are vectors that represent the words in the context we are considering. They act like an index or a reference. <strong>Values</strong> are the actual word embeddings or features that we use to generate the output. They contain the information we are interested in.</p> <p>So at this point, knowing the tokens, we can implement some form of linear transformation with some given weights in the transformation matrices (which they need to be calculated during the training process) to find the queries, keys, and values as follows (Don’t worry too much about what they mean. Juts get a sense of how they can be calculated from input word embeddings. Things become more clear moving forward.)</p> \[Q_i=W_Q\times x_i \quad, \quad K_i=W_K\times x_i \quad , \quad V_i=W_V\times x_i\] <p>Here, \(x_i\) is the <em>vector space</em> for each token, i.e. each row in Matrix \(X\) (in previous section) and \(W\)’s are <strong>weight matrices that need to be found during the training</strong>. Also,</p> \[Q_i=[q_1, q_2, q_3, ...] \quad , \quad K_i=[k_1, k_2, k_3, ...] \quad, \quad \text{and} \quad V_i=[v_i, v_2, v_3,...]\] <p>where \(q_i\), \(k_i\), and \(v_i\) are each a vector.</p> <p>This matrix representation is basically telling you that you will have one query vector, one key vector, and one value vector for each word embedding vector. In another word, now we have three vector representations for each word embedding vector.</p> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/qkv.png" alt="queries-keys-values" width="900" height="250"> </div> <h3 id="self-attention-mechanism-">Self-Attention Mechanism <a name="selfatt"></a> </h3> <p>This mechanism calculates a score between each query and every key to determine how much <em>weight</em> should be given to each word. This weight is usually computed as a dot product of the query and key vectors. We typically apply a softmax to the outcome to keep things under control!</p> \[\text{Attention Weights}=Softmax[K^TQ]\] <p>The weighted values are then calculated using</p> \[\text{Weighted Values}=V.Softmax[K^TQ]\] <p>Let me try to put everything together so you can get a better picture of the general procedure for the case of an input with three tokens:</p> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/attention.png" alt="attention-mechanism" width="700" height="250"> </div> <p>So at this point, the output for each token incorporates information from all other tokens in the sequence, weighted by their relevance. This means that the representation of token \(i\) is not just based on the token itself but it is also influenced by how much attention it gives to other tokens.</p> <p><strong>NOTE:</strong> In order to deal with large magnitudes in the dot product operation, when calculating the weights, we typically scale the dot product as \(\text{Weighted Values}=V.Softmax[\frac{K^TQ}{\sqrt{D_q}}]\), where \(D_q\) is the dimension of the queries. This scaling procedure when dealing with large magnitudes in attention computation is important since</p> <blockquote> <p>“Small changes to the inputs to the softmax function might have little effect on the output (i.e. the gradients are very small), making the model difficult to train”. ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>We can also write a simple Python code for this procedure and test it out:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
	
	<span class="n">d_k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Dimension of queries and keys
</span>	<span class="n">d_v</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Dimension of values
</span>	
	<span class="c1"># Example word representations (for simplicity, these are random)
</span>	<span class="n">queries</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># 3 words, d_k dimensions
</span>	<span class="n">keys</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>    <span class="c1"># 3 words, d_k dimensions
</span>	<span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>   <span class="c1"># 3 words, d_v dimensions
</span>	
	<span class="c1"># Calculating the dot product between queries and keys (transpose keys)
</span>	<span class="n">dot_products</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Dot Products:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">dot_products</span><span class="p">)</span>
	
	<span class="c1"># Applying softmax to get attention weights
</span>	<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
	
	<span class="n">attention_weights</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">dot_products</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention Weights:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">)</span>
	
	<span class="c1"># Multiplying the attention weights with the values
</span>	<span class="n">weighted_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">weighted_values</span><span class="p">)</span>

</code></pre></div></div> <p>returning</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	Attention Weights:
	 [[0.46831053 0.06337894 0.46831053]
	 [0.4223188  0.4223188  0.1553624 ]
	 [0.57611688 0.21194156 0.21194156]]
	 
	Weighted Values:
	 [[4.         5.         6.        ]
	 [3.19913082 4.19913082 5.19913082]
	 [2.90747402 3.90747402 4.90747402]]
</code></pre></div></div> <p>If things are still not clear, take a look at the <a href="#struc-example">Attention Mechanism: Learn by Example</a> section, where I went through this procedure with an example calculations for a few tokens in a given sentence.</p> <h3 id="normalization">Normalization<a name="norm"></a> </h3> <p>This mechanism, like <em>Batch Normalization</em>, is commonly used in neural networks to stabilize and accelerate the training process. The idea is to normalize the input of each layer so that it has a mean of zero and a variance of one. This helps in mitigating the internal covariate shift problem. Here is how Batch Normalization works:</p> <ol> <li> <strong>Calculate Mean and Variance</strong>: For a given batch, calculate the mean and variance of the inputs.</li> <li> <strong>Normalize</strong>: Subtract the mean and divide by the standard deviation to normalize the inputs.</li> <li> <strong>Scale and Shift</strong>: Apply learned scaling and shifting parameters to the normalized inputs.</li> </ol> <h3 id="positional-encoding">Positional Encoding<a name="posenc"></a> </h3> <p>Positional encoding provides information about the position of each word in the sequence. This is essential because unlike recurrent or convolutional layers, transformers do not have a built-in notion of sequence order.</p> <p>Positional encodings are added to the input embeddings to give the model some information about the relative or absolute position of the tokens. The encoding matrix can be created by hand or can be learned. It can be added to the network inputs or at every network layer.</p> <p>One common approach is to use sine and cosine functions of different frequencies. The idea is to generate unique positional encodings that the model can learn to interpret.</p> <p>For even indices:</p> \[\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\] <p>For the odd indices:</p> \[\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\] <p>where</p> <ul> <li>\(pos\) is the position in the sequence,</li> <li>\(i\) is the dimension,</li> <li>\(d_{model}\)​ is the dimension of the model, i.e. size of the vector space in which the tokens are represented.</li> </ul> <p>Here’s a simple implementation of positional encoding in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
	<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
	<span class="n">scaling_factor</span> <span class="o">=</span> <span class="mi">10000</span>
	
	<span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
	    <span class="n">pos_enc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
	    <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
	        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
	            <span class="n">pos_enc</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="n">scaling_factor</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
	            <span class="n">pos_enc</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="p">(</span><span class="n">scaling_factor</span> <span class="o">**</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)))</span>
	    <span class="k">return</span> <span class="n">pos_enc</span>
	
	<span class="c1"># Example usage
</span>	<span class="n">max_len</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Maximum length of the sequence
</span>	<span class="n">d_model</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1"># Dimension of the model
</span>	
	<span class="n">pos_encoding</span> <span class="o">=</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Positional Encoding Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">pos_encoding</span><span class="p">)</span>
	
	<span class="c1"># Visualize the positional encoding
</span>	<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Depth</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Position</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Positional Encoding</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;"> <img src="/assets/img/pencoding.png" alt="positional-encoding" width="500" height="300"> </div> <p>In the output figure, you can see that for any given position across the dimensions, we have a unique set of color combination.</p> <h2 id="3-attention-mechanism-learn-by-example--">3. Attention Mechanism: Learn by Example <a name="struc-example"></a> </h2> <p>In this section, I will try to focus on detailed calculations for the attention mechanism, hoping that it might help those who are more comfortable with actually seeing sample calculations when it comes to understanding a concept. Let’s use the following sentence:</p> <blockquote> <p>Despite the heavy rain, the children played happily in the park, unaware of the approaching storm.</p> </blockquote> <h3 id="tokenization">Tokenization</h3> <p>First, let’s tokenize the sentence into individual words or subwords:</p> <p>Tokens=[ “Despite”, “the” , “heavy” , “rain”, “,” , “the” , “children” , “played” , “happily” , “in” , “the” , “park” , “,” , “unaware” , “of” , “the” , “approaching” , “storm” , “.” ]</p> <h3 id="embedding">Embedding</h3> <p>We then map the tokens into a vector in a high-dimensional space. This mapping is achieved using a pre-trained embedding matrix (e.g., Word2Vec, GloVe, or embeddings learned as part of a transformer model like BERT or GPT).</p> <p>Let’s use a simplified example where each token is mapped to a 4-dimensional vector. In practice, these vectors would be of higher dimensions (e.g., 300, 768, 1024).</p> <p>Here’s an example of what this might look like (with randomly chosen values for illustration):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Despite"→[0.2,−0.1,0.5,0.3]
"the"→[0.1,0.0,−0.1,0.4]
"heavy"→[−0.3,0.8,0.1,0.2]
"rain"→[0.4,0.3,−0.2,0.1]
","→[0.0,0.0,0.0,0.0]
"children"→[0.5,0.2,0.6,−0.1]
"played"→[0.3,0.1,0.4,0.7]
"happily"→[−0.2,0.5,−0.3,0.4]
"in"→[0.1,−0.3,0.2,0.5]
"park"→[0.4,0.6,0.1,−0.4]
"unaware"→[0.2,0.7,−0.5,0.1]
"of"→[0.1,0.0,0.3,−0.2]
"approaching"→[0.3,0.4,0.6,0.2]
"storm"→[0.5,−0.1,0.4,0.3]
"."→[0.0,0.0,0.0,0.0]
</code></pre></div></div> <p>These vectors (embeddings) are typically learned from large corpora of text and capture semantic meanings. For example, “rain” and “storm” might have similar embeddings because they both relate to weather.</p> <h3 id="building-the-embedding-matrix">Building the Embedding Matrix</h3> <p>For our sentence, we create an embedding matrix where each row corresponds to the embedding of a token. If our sentence has 19 tokens and each token is embedded in a 4-dimensional space, our embedding matrix $E$ would be of size \(19\times4\):</p> <table> <thead> <tr> <th>Token</th> <th>Dimension 1</th> <th>Dimension 2</th> <th>Dimension 3</th> <th>Dimension 4</th> </tr> </thead> <tbody> <tr> <td>Despite</td> <td>0.2</td> <td>-0.1</td> <td>0.5</td> <td>0.3</td> </tr> <tr> <td>the</td> <td>0.1</td> <td>0.0</td> <td>-0.1</td> <td>0.4</td> </tr> <tr> <td>heavy</td> <td>-0.3</td> <td>0.8</td> <td>0.1</td> <td>0.2</td> </tr> <tr> <td>rain</td> <td>0.4</td> <td>0.3</td> <td>-0.2</td> <td>0.1</td> </tr> <tr> <td>,</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> </tr> <tr> <td>children</td> <td>0.5</td> <td>0.2</td> <td>0.6</td> <td>-0.1</td> </tr> <tr> <td>played</td> <td>0.3</td> <td>0.1</td> <td>0.4</td> <td>0.7</td> </tr> <tr> <td>happily</td> <td>-0.2</td> <td>0.5</td> <td>-0.3</td> <td>0.4</td> </tr> <tr> <td>in</td> <td>0.1</td> <td>-0.3</td> <td>0.2</td> <td>0.5</td> </tr> <tr> <td>the</td> <td>0.1</td> <td>0.0</td> <td>-0.1</td> <td>0.4</td> </tr> <tr> <td>park</td> <td>0.4</td> <td>0.6</td> <td>0.1</td> <td>-0.4</td> </tr> <tr> <td>unaware</td> <td>0.2</td> <td>0.7</td> <td>-0.5</td> <td>0.1</td> </tr> <tr> <td>of</td> <td>0.1</td> <td>0.0</td> <td>0.3</td> <td>-0.2</td> </tr> <tr> <td>the</td> <td>0.1</td> <td>0.0</td> <td>-0.1</td> <td>0.4</td> </tr> <tr> <td>approaching</td> <td>0.3</td> <td>0.4</td> <td>0.6</td> <td>0.2</td> </tr> <tr> <td>storm</td> <td>0.5</td> <td>-0.1</td> <td>0.4</td> <td>0.3</td> </tr> <tr> <td>.</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> <td>0.0</td> </tr> </tbody> </table> <h3 id="queries-keys-and-values">Queries, Keys, and Values</h3> <p>For illustration purposes, let’s define our weight matrices with arbitrary values</p> \[W_Q=\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4\\ 0.5 &amp; 0.6 &amp; 0.7 &amp; 0.8\\ 0.9 &amp; 1.0 &amp; 1.1 &amp; 1.2\\ 1.3 &amp; 1.4 &amp; 1.5 &amp; 1.6 \end{bmatrix}\] \[W_K=\begin{bmatrix} 1.6 &amp; 1.5 &amp; 1.4 &amp; 1.3\\ 1.2 &amp; 1.1 &amp; 1.0 &amp; 0.9\\ 0.8 &amp; 0.7 &amp; 0.6 &amp; 0.5\\ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \end{bmatrix}\] \[W_V=\begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; -0.4\\ -0.5 &amp; 0.6 &amp; -0.7 &amp; 0.8\\ 0.9 &amp; -1.0 &amp; 1.1 &amp; -1.2\\ -1.3 &amp; 1.4 &amp; -1.5 &amp; 1.6 \end{bmatrix}\] <p>and use them to calculate queries, keys, and values for the first three tokens</p> <ul> <li>“Despite” with embedding [0.2, −0.1, 0.5, 0.3]:</li> </ul> \[Q_{Despite}=W_Q\begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4\\ 0.5 &amp; 0.6 &amp; 0.7 &amp; 0.8\\ 0.9 &amp; 1.0 &amp; 1.1 &amp; 1.2\\ 1.3 &amp; 1.4 &amp; 1.5 &amp; 1.6 \end{bmatrix} \quad\times\quad \begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.27 \\ 0.63 \\ 0.99 \\ 1.35\end{bmatrix}\] \[K_{Despite}=W_K\begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 1.6 &amp; 1.5 &amp; 1.4 &amp; 1.3\\ 1.2 &amp; 1.1 &amp; 1.0 &amp; 0.9\\ 0.8 &amp; 0.7 &amp; 0.6 &amp; 0.5\\ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \end{bmatrix} \quad\times\quad \begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 1.26 \\ 0.9 \\ 0.54 \\ 0.18\end{bmatrix}\] \[V_{Despite}=W_V\begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; -0.4\\ -0.5 &amp; 0.6 &amp; -0.7 &amp; 0.8\\ 0.9 &amp; -1.0 &amp; 1.1 &amp; -1.2\\ -1.3 &amp; 1.4 &amp; -1.5 &amp; 1.6 \end{bmatrix} \quad\times\quad \begin{bmatrix} 0.2 \\ -0.1 \\ 0.5 \\ 0.3 \end{bmatrix}= \begin{bmatrix} 0.07 \\ -0.27 \\ 0.47 \\ -0.67\end{bmatrix}\] <ul> <li>“the” with embedding [0.1, 0.0, −0.1, 0.4] and “heavy” with embedding [−0.3, 0.8, 0.1, 0.2]:</li> </ul> \[Q_{the}=W_Q\begin{bmatrix} 0.1 \\ 0.0 \\ -0.1 \\ 0.4 \end{bmatrix}= \begin{bmatrix} 0.14 \\ 0.3 \\ 0.46 \\0.62\end{bmatrix} \quad, \quad Q_{heavy}=W_Q\begin{bmatrix} -0.3 \\ 0.8 \\ 0.1 \\ 0.2 \end{bmatrix}= \begin{bmatrix} 0.24 \\ 0.56 \\ 0.88 \\ 1.2 \end{bmatrix}\] \[K_{the}=W_K\begin{bmatrix} 0.1 \\ 0.0 \\ -0.1 \\ 0.4 \end{bmatrix}= \begin{bmatrix} 0.54 \\ 0.38 \\ 0.22 \\ 0.06\end{bmatrix} \quad, \quad K_{heavy}=W_K\begin{bmatrix} -0.3 \\ 0.8 \\ 0.1 \\ 0.2 \end{bmatrix}= \begin{bmatrix} 1.12 \\ 0.8 \\ 0.48 \\ 0.16\end{bmatrix}\] \[V_{the}=W_V\begin{bmatrix} 0.1 \\ 0.0 \\ -0.1 \\ 0.4 \end{bmatrix}= \begin{bmatrix} -0.18 \\ 0.34 \\ -0.5 \\ 0.66\end{bmatrix} \quad, \quad V_{heavy}=W_V\begin{bmatrix} -0.3 \\ 0.8 \\ 0.1 \\ 0.2 \end{bmatrix}= \begin{bmatrix} -0.24 \\ 0.72 \\ -1.2 \\ 1.68\end{bmatrix}\] <p>By applying the learned weight matrices $W_Q$​, $W_K$​, and $W_V$, we transform the original embeddings into queries, keys, and values. These vectors are then used in the attention mechanism to compute attention scores and to derive contextually rich representations of each token. Let’s see how this works.</p> <h3 id="computing-attention-weights">Computing Attention Weights</h3> <p>For each token, we compute the dot product of its query vector with the key vectors of all tokens. This results in the attention scores for each token relative to every other token.</p> <p>Let’s continue with the example of “Despite” with</p> \[Q_{Despite}=\begin{bmatrix} 0.27 \\ 0.63 \\ 0.99 \\ 1.35\end{bmatrix}\] <p>and then find the attention scores between “Despite” and three other tokens, including itself (“the”, “heavy”, and “Despite” ) with</p> \[K_{the}=\begin{bmatrix} 0.54 \\ 0.38 \\ 0.22 \\ 0.06\end{bmatrix} \quad , \quad K_{heavy}=\begin{bmatrix} 1.12 \\ 0.8 \\ 0.48 \\ 0.16\end{bmatrix} \quad , \quad K_{Despite}=\begin{bmatrix} 1.26 \\ 0.9 \\ 0.54 \\ 0.18\end{bmatrix}\] <ol> <li>Dot product between “Despite” and “the”: \(Dot(Q_{Despite}, K_{the})= 0.1458 + 0.2394 + 0.2178 + 0.081 = 0.684\)</li> <li>Dot product between “Despite” and “heavy”: \(Dot(Q_{Despite}, K_{heavy})= 0.3024 + 0.504 + 0.4752 + 0.216 = 1.4976\)</li> <li>Dot product between “Despite” and “Despite” (self-attention): \(Dot(Q_{Despite}, K_{Despite})= 0.3402 + 0.567 + 0.5346 + 0.243 = 1.6848\)</li> </ol> <p>These dot products represent the raw attention weights for “Despite” in relation to “the”, “heavy,” and itself.</p> \[\text{Softmax}([0.684, 1.4976, 1.6848]) = \left[ \frac{e^{0.684}}{e^{0.684} + e^{1.4976} + e^{1.6848}}, \frac{e^{1.4976}}{e^{0.684} + e^{1.4976} + e^{1.6848}}, \frac{e^{1.6848}}{e^{0.684} + e^{1.4976} + e^{1.6848}} \right]\] <p>These softmax values represent the normalized attention scores for “Despite” with respect to “the,” “heavy,” and itself, respectively. They indicate the relative importance of “Despite” compared to the other tokens during the attention mechanism.</p> <h2 id="4-transformer-models-">4. Transformer Models <a name="models"></a> </h2> <p>We can generally classify transformers into three models, <strong>Encoders</strong>, <strong>Decoders</strong>, and <strong>Encoder-Decoders</strong>. An encoder:</p> <blockquote> <p>“…transforms the text embeddings into a representation that can support variety of tasks.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>A decoder, however, is typically used to generate the next output and to continue the given input text, like GPT models.</p> <p>Finally, encoder-decoders are implemented in</p> <blockquote> <p>“…sequence-to-sequence tasks, where one text string is converted into another.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>A common example for encoder-decoder model is language translation.</p> <h2 id="5-coding-a-simple-transformer-">5. Coding a Simple Transformer <a name="struc"></a> </h2> <p>In this section, we will try to develop a simple transformer piece-by-piece using Python. This simple model includes an embedding layer, positional encoding, attention mechanism, a feed-forward neural network, normalization layer, as well as encoder and decoder parts. Keep in mind that a typical Transformer has a much more complex structure with a extra components (such as residual connections and <a href="https://aiml.com/explain-self-attention-and-masked-self-attention-as-used-in-transformers/" rel="external nofollow noopener" target="_blank">masked attention</a>) and it also is implemented through a <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html" rel="external nofollow noopener" target="_blank"><em>multi-headed</em></a> approach for parallel computation. Here, <strong>the goal is to have a better understanding of how the building blocks of a Transformer piece together.</strong></p> <h3 id="embedding-layer">Embedding Layer</h3> <p>Let’s start with the embedding layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">EmbeddingLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding_matrix</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>

</code></pre></div></div> <p>Here, we use a simple example, the sentence “I love learning from examples” with a given vocabulary, and see how this embedding layer works:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining the vocabulary and input sentence
</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">I</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">learning</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">from</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">examples</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="n">input_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">I</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">learning</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Converting the input sentence to indices
</span><span class="n">input_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_sentence</span><span class="p">]</span>

<span class="c1"># Initializing the embedding layer
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Let's use 4 dimensions for simplicity
</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="nc">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="c1"># Getting the embeddings for the input indices
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="nf">embedding_layer</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">input_indices</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Indices:</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embeddings:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre></div></div> <p>The above code will give the following output (yours will be different since I used <code class="language-plaintext highlighter-rouge">np.random.rand</code> in the <code class="language-plaintext highlighter-rouge">EmbeddingLayer</code> class so every run gives a different set of outputs):</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Indices: <span class="o">[</span>0, 1, 2]

Embeddings:
 <span class="o">[[</span>0.90034368 0.82241612 0.02018069 0.62033932]
 <span class="o">[</span>0.6351551  0.33107626 0.78112305 0.21081683]
 <span class="o">[</span>0.93584476 0.08042298 0.05619254 0.74456291]]
</code></pre></div></div> <h3 id="positional-encoding-1">Positional Encoding</h3> <p>We continue our model development by creating a positional encoding layer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">encoding</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

</code></pre></div></div> <p>Here is the implementation of the class using the example sentence in the previous section:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pos_encoding_layer</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> 
<span class="n">embeddings_with_positional_encoding</span> <span class="o">=</span> <span class="nf">pos_encoding_layer</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:])</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Embeddings with Positional Encoding:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">embeddings_with_positional_encoding</span><span class="p">)</span>
</code></pre></div></div> <p>which will return (yours will be different since I used <code class="language-plaintext highlighter-rouge">np.random.rand</code> in the <code class="language-plaintext highlighter-rouge">EmbeddingLayer</code> class so every run gives a different set of outputs)</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Embeddings with Positional Encoding:
 <span class="o">[[[</span> 0.90034368  1.82241612  0.02018069  1.62033932]
  <span class="o">[</span> 1.47662609  0.87137857  0.79112288  1.21076683]
  <span class="o">[</span> 1.84514219 <span class="nt">-0</span>.33572386  0.07619121  1.74436292]]]
</code></pre></div></div> <p>In the output, the positional encoding are those same vectors as embeddings with positional information added. The positional encoding modifies the embeddings to incorporate the position of each word in the sentence, making it easier for the model to learn the order of words.</p> <h3 id="self-attention-mechanism">Self-Attention Mechanism</h3> <p>Once we have our embeddings, we need to have our self attention layer, which can be coded as follows</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_q</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_k</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_v</span><span class="p">)</span>
        
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_o</span><span class="p">)</span>
</code></pre></div></div> <p>Again, let us look at how it operates using a simple example, assuming a sentence with 3 tokens, for a 4-dimensional model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A simple input matrix (3 words with 4-dimensional embeddings)
</span><span class="n">input_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Dimensionality of the input
</span><span class="n">self_attention_layer</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">output_matrix</span> <span class="o">=</span> <span class="nf">self_attention_layer</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Matrix after Self-Attention:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">)</span>
</code></pre></div></div> <p>The above input example will return (yours will be different since I used <code class="language-plaintext highlighter-rouge">np.random.rand</code> in the <code class="language-plaintext highlighter-rouge">EmbeddingLayer</code> class so every run gives a different set of outputs)</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Matrix:
 <span class="o">[[</span>1 0 1 0]
 <span class="o">[</span>0 1 0 1]
 <span class="o">[</span>1 1 1 1]]
 
Output Matrix after Self-Attention:
 <span class="o">[[</span>4.00438965 3.73163309 4.01682177 3.50892256]
 <span class="o">[</span>4.08366923 3.80766097 4.10031155 3.5832559 <span class="o">]</span>
 <span class="o">[</span>4.37889227 4.08744366 4.40506536 3.85251859]]
</code></pre></div></div> <p>In the output, the input matrix represents three words with four features each. And the output matrix shows the result of applying the self-attention mechanism to the input matrix, essentially showing how each word’s representation is influenced by the others in the sequence.</p> <p>We need two more layers, one fully-connected feed-forward neural network and one normalization layer.</p> <h3 id="feed-forward-neural-network">Feed-Forward Neural Network</h3> <p>This layer is just a typical fully connected neural network. The purpose of the feed-forward network is to transform the attention output.</p> <p>The ReLU activation introduces non-linearity, allowing the model to learn more complex patterns and representations beyond what linear transformations can capture. Further, this layer can help capture richer features and improve the model’s ability to generalize. Finally, this layer allows the model to process each token’s representation separately after attending to other tokens, enhancing the model’s capacity to learn individual token features.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
	    <span class="c1"># d_model: dimension of the input and output (same as the input embedding dimension).
</span>	    <span class="c1"># d_ff: dimension of the hidden layer in the feedforward network.
</span>	    
		<span class="c1"># A weight matrix of shape `(d_model, d_ff)` initialized with random values.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
		<span class="c1"># A weight matrix of shape `(d_ff, d_model)` initialized with random values.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
	    <span class="c1"># `np.maximum(0, np.dot(x, self.W1))`: applies the ReLU activation function to introduce non-linearity.
</span>	    <span class="c1"># `np.dot(np.maximum(0, np.dot(x, self.W1)), self.W2)`: projects the hidden layer back to the original dimension `d_model`.
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W1</span><span class="p">)),</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">)</span>

</code></pre></div></div> <h3 id="layer-normalization">Layer Normalization</h3> <p>The <code class="language-plaintext highlighter-rouge">LayerNorm</code> class implements layer normalization. The layer normalization is applied to the inputs of the sub-layers in the Transformer architecture to stabilize and accelerate the training process. In the transformer architecture, layer normalization is applied at various points, typically before and after the main sub-layers (self-attention and feed-forward neural network).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>

</code></pre></div></div> <h3 id="encoder-layer">Encoder Layer</h3> <p>Now, putting all these components together, we can define a simple <strong>Encoder</strong> structure as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div></div> <p>And here is an example code to test the procedure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple example input matrix (3 words with 4-dimensional embeddings) 
</span><span class="n">input_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]])</span> 

<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Dimension of the input 
</span><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># Dimension of the hidden layer in the feedforward network 
</span><span class="n">encoder_layer</span> <span class="o">=</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span> 

<span class="c1"># Applying the EncoderLayer to the input matrix 
</span><span class="n">output_matrix</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">)</span> 

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_matrix</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Matrix after EncoderLayer:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">)</span>
</code></pre></div></div> <p>The final output matrix (<code class="language-plaintext highlighter-rouge">output_matrix</code>) contains the transformed embeddings of the input tokens. Each row in this matrix represents a token in the sequence, but now these token representations are context-aware and enriched with information from the entire sequence.</p> <p>The potential applications of the output are</p> <ul> <li> <strong>Context-Awareness</strong>: Each token’s representation in the output matrix is affected by other tokens in the sequence, capturing some of the dependencies and relationships.</li> <li> <strong>Foundation for Further Processing</strong>: This output serves as the input for subsequent layers in the Transformer model, building progressively richer representations that can be used for tasks like translation, classification, or other sequence-to-sequence tasks.</li> </ul> <h3 id="decoder-layer">Decoder Layer</h3> <p>We can also construct a <strong>Decoder</strong> using the concepts and codes we previously introduced:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>
        <span class="c1"># Self-attention on the decoder's own input
</span>        <span class="n">self_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Cross-attention with the encoder's output
</span>        <span class="n">enc_dec_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">enc_dec_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">enc_dec_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Feedforward network
</span>        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

</code></pre></div></div> <p>Here, there is a new concept called <strong>Cross-Attention</strong> (or encoder-decoder attention).</p> <p>Remember that a Transformer pays attention to various tokens in a given sentence using self-attention mechanism. However, when we have a decoder, paying attention to the tokens in a sentence in the decoder is not enough, we also need to pay attention to the encoder’s outputs. For instance in a language translation procedure, when we couple an encoder with a decoder, the model needs to pay attention to the words in the translated sentence in the output language (self-attention) <strong>and</strong> the words in the original language (cross-attention). Therefore, using this cross-attention layer, decoder embeddings attend to the encoder embeddings. In this scenario,</p> <blockquote> <p>“… the queries are computed from the decoder embeddings and the keys and values from the encoder embeddings.” ~ Simon J.D. Prince, “Understanding Deep Learning”</p> </blockquote> <p>Let us then take a look at a simple example of a decoder along with an example of how it takes a set of inputs and returns an output which is a final transformed matrix, capturing the relationships between the decoder’s input tokens and the encoder’s output tokens. This matrix is then can be passed to the next layer of the Transformer decoder or to be used for generating the output sequence.</p> <p>Putting it all together with an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_q</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_k</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_v</span><span class="p">)</span>
        
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W_o</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W1</span><span class="p">)),</span> <span class="n">self</span><span class="p">.</span><span class="n">W2</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>

<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">enc_dec_attention</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm3</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">):</span>
        <span class="c1"># Self-attention on the decoder's own input
</span>        <span class="n">self_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Cross-attention with the encoder's output
</span>        <span class="n">enc_dec_attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">enc_dec_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">enc_dec_attn_output</span><span class="p">)</span>
        
        <span class="c1"># Feedforward network
</span>        <span class="n">ff_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Example input matrix (decoder's input)
</span><span class="n">decoder_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> 
                          <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> 
                          <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]])</span>

<span class="c1"># Example encoder output matrix
</span><span class="n">encoder_output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> 
                           <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> 
                           <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">]])</span>

<span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Dimensionality of the input
</span><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">8</span>     <span class="c1"># Dimensionality of the hidden layer in the feedforward network
</span><span class="n">decoder_layer</span> <span class="o">=</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

<span class="n">output_matrix</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoder Input Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Encoder Output Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Matrix after DecoderLayer:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">)</span>
</code></pre></div></div> <h2 id="6-final-comments-">6. Final Comments <a name="fincom"></a> </h2> <p>The main place to experiment with various Transformer-based models, used for NLP, Computer Vision, and automatic speech recognition is the <a href="https://huggingface.co/docs/transformers/en/index" rel="external nofollow noopener" target="_blank">Hugging Face</a>. I strongly suggest joining the community and learn by following tutorials and implementing models for your own projects.</p> <p>To have a much better understanding of how the mechanics of Transformers for NLP-related tasks work along with learning how to implement various NLP-related tasks, I suggest looking at <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="external nofollow noopener" target="_blank">Andrej Karpathy YouTube channel</a>, specially <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=8" rel="external nofollow noopener" target="_blank">Let’s build GPT: from scratch, in code, spelled out.</a> and <a href="https://www.youtube.com/watch?v=l8pRSuU81PU&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=11" rel="external nofollow noopener" target="_blank">Let’s reproduce GPT-2 (124M)</a>.</p> <p>If you are looking for more comprehensive and clear discussion about Transformers and their working principles, I suggest <a href="https://issuu.com/cmb321/docs/deep_learning_ebook" rel="external nofollow noopener" target="_blank">Deep Learning: Foundations and Concepts</a> by <em>Christopher M. Bishop</em> and <em>Hugh Bishop</em> (chapter 12) and also <a href="https://github.com/udlbook/udlbook/releases/download/v4.0.1/UnderstandingDeepLearning_05_27_24_C.pdf" rel="external nofollow noopener" target="_blank">Understanding Deep Learning</a> by <em>Simon J.D. Prince</em> (chapter 12).</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/svd/">Dynamic Mode Decomposition for Fluid Dynamics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/pinn-1dheat/">TPhysics Informed Neural Network for 1D Heat Transfer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/text-to-vector-with-milvus/">Turning Text into a Vector Database with Milvus</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/execution/">Ideas Matter More Than Ever</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/problem-solving/">Data-Driven vs. First-Principles</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023-2025 Masoud Masoumi. Modified by Masoud Masoumi based on the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>